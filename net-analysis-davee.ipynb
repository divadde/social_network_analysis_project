{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1632066,"sourceType":"datasetVersion","datasetId":935914},{"sourceId":8969929,"sourceType":"datasetVersion","datasetId":5400085},{"sourceId":8974047,"sourceType":"datasetVersion","datasetId":5403002}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importazione librerie e visualizzazione Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nfrom itertools import combinations\nimport matplotlib.pyplot as plt","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lettura del dataset:\ndf_trump = pd.read_csv(\"/kaggle/input/us-election-2020-tweets/hashtag_donaldtrump.csv\",lineterminator='\\n')\ndf_biden = pd.read_csv(\"/kaggle/input/us-election-2020-tweets/hashtag_joebiden.csv\",lineterminator='\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Tweet with Trump hashtag: {len(df_trump)}\")\nprint(f\"Tweet with Biden hashtag: {len(df_biden)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataframe unito (eliminati i duplicati)\ndf_duplicated = pd.concat([df_trump,df_biden])\ndf = df_duplicated.drop_duplicates(subset=\"tweet\")\n\nprint(f\"Total tweets: {len(df_duplicated)}\")\nprint(f\"Total tweets: {len(df)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Numero di utenti totali (potenziali nodi)\nprint(df[\"user_id\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport re\n\ndef extract_hashtags(tweet):\n    return re.findall(r'#\\w+', tweet.lower())\n\ndf['hashtags'] = df['tweet'].apply(extract_hashtags)\n\nall_hashtags = [hashtag for hashtags in df['hashtags'] for hashtag in hashtags]\n\nhashtag_counts = Counter(all_hashtags)\n\nsorted_hashtag_counts = hashtag_counts.most_common()\n\n# Stampare la classifica degli hashtag\nprint(\"Classifica degli hashtag più usati:\")\nfor hashtag, count in sorted_hashtag_counts[:50]:\n    print(f\"{hashtag}: {count}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Osservazioni:\n- Informazioni temporali che vanno dal 15 ottobre 2020 al 8 novembre 2020.\n- 481.000 potenziali nodi (filtraggio sulla base di like/retweet?)\n- Tweet scritti in diverse lingue (concentrarsi solo su quelli in inglese?)\n- Diversi valori mancanti nelle aree geografiche","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing (filtraggio tweet/utenti)","metadata":{}},{"cell_type":"markdown","source":"Probabilmente il primo filtraggio che occorre fare è quello sulla lingua. Potrebbe essere meglio considerare solo i tweet in inglese (?)","metadata":{}},{"cell_type":"code","source":"#Filtraggio sulla base dei like\ndf_like_5 = df[df[\"likes\"]>=5]\ndf_like_10 = df[df[\"likes\"]>=10]\ndf_like_20 = df[df[\"likes\"]>=20]\ndf_like_50 = df[df[\"likes\"]>=50]\n\nprint(f\"Total tweets: {len(df_like_5)}\")\nprint(f\"Total tweets: {len(df_like_10)}\")\nprint(f\"Total tweets: {len(df_like_20)}\")\nprint(f\"Total tweets: {len(df_like_50)}\")\nprint(df_like_50[\"user_id\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filtraggio sulla base dei retweet\ndf_retweet_5 = df[df[\"retweet_count\"]>=5]\ndf_retweet_10 = df[df[\"retweet_count\"]>=10]\ndf_retweet_20 = df[df[\"retweet_count\"]>=20]\ndf_retweet_50 = df[df[\"retweet_count\"]>=50]\n\nprint(f\"Total tweets: {len(df_retweet_5)}\")\nprint(f\"Total tweets: {len(df_retweet_10)}\")\nprint(f\"Total tweets: {len(df_retweet_20)}\")\nprint(f\"Total tweets: {len(df_retweet_50)}\")\nprint(df_retweet_50[\"user_id\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#FILTRAGGIO BASATO SU paese=United states\ndf_country= df[df[\"country\"]==\"United States of America\"]\nprint(f\"Total tweets: {len(df_country)}\")\n\nprint(df_country[\"user_id\"].value_counts())\ndf_country.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classificazione preferenze (pro-Trump or pro-Biden) con llama3","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n!pip install langchain-community\n!pip install langchain-core\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh\n    \n#Avvio del server locale di Ollama\nt = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\nt.start()\n\n!ollama pull llama3\n\nt2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\nt2.start()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_community.llms import Ollama\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = \"You are a political classifier over a list of tweets about USA election. Your role is to analyze the list of tweets of users and to establish if user is Pro-Biden or Pro-Trump. Each tweet start when you read: 'TWEET START'. You must assign TO EACH tweet a class (Pro-Biden or Pro-Trump). You have to give ONLY the class for EACH tweet, NOT ANYMORE. The class for each tweet must be separated by a comma. If a tweet has offensive language, ignore it and predict the class for this tweet as 'X'.\"\n\nllm = Ollama(\n    model=\"llama3\"\n)  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", prompt),\n    (\"user\", \"{input}\"),\n])\n\noutput_parser = StrOutputParser()\n\n\ndef preference_llama(tweet):   \n    #chain = template | llm | output_parser\n    \n    #response = chain.invoke({\"input\": \"Tweet 1:\" +tweet1+ \". Tweet 2:\" +tweet2})\n    response = llm.invoke(prompt + \"Tweet:\" + tweet)\n    \n    return response","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport time\n\n# Specifica il nome del file JSON\nfilename = '/kaggle/working/preferences.json'\nrecords = []\n\nfor index, row in df_sampled.iterrows(): \n    resp = preference_llama(row.tweet)\n    print(resp)\n    record = {\n        \"user\": row.user_screen_name,\n        \"class\": resp\n    }\n    records.append(record)\n\n\"\"\"\n#prova con lista di tweet\ntweet_list=[]\ncounter=0\nmax_list=4\nfor index, row in df_sampled.iterrows(): \n    #print(row)\n    tweet_list.append(row)\n    if counter<max_list-1:\n        counter=counter+1\n    else:\n        counter2=0\n        tweets=\"TWEET START: \"\n        for row in tweet_list:\n            counter2=counter2+1\n            if(counter2==counter):\n                tweets=tweets+row.tweet+\".\"\n            else:\n                tweets=tweets+row.tweet+\". TWEET START:\"\n        resp = preference_llama(tweets,prompt)\n        print(resp)\n        for row in tweet_list:\n            record = {\n                \"user\": row.user_screen_name,\n                \"class\": resp\n            }\n            records.append(record)\n        counter=0\n        tweet_list.clear()\n\"\"\"\n    \n# Scrivi i dati nel file JSON\nwith open(filename, 'w') as file:\n    json.dump(records, file)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Costruisco la rete di similarità con gli hashtag","metadata":{}},{"cell_type":"markdown","source":"Si pone il seguente problema: potrebbe non essere la scelta giusta andare a escludere utenti per numero di followers. Da un lato potremmo escludere il comportamento tipico degli utenti meno popolari, che sono anche quelli più numerosi (le persone comuni, che poi di fatto vanno a votare), dall'altro potremmo escludere il ruolo di utenti più popolari in grado di influenzare maggiormente gli altri utenti. Potremmo pensare di effettuare un campionamento casuale dei nodi per ridurre la dimensione della rete. Oppure dovremmo pensare al filtraggio sotto altri metodi (numero di like o retweet?). Potremmo fare anche un campionamento che si basa sulla degree distribution. Probabilmente la cosa migliore è andare a fare un campionamento casuale direttamente sul dataset.","metadata":{}},{"cell_type":"markdown","source":"Provo invece a considerare i top 100 e i last 1000.","metadata":{}},{"cell_type":"code","source":"#raggruppo solo per followers\ngrouped_followers = df_country.groupby('user_screen_name').agg({'user_followers_count':'first'}).reset_index()\nprint(f\"Total tweets after concate: {len(grouped_followers)}\")\nprint(grouped_followers[\"user_screen_name\"].value_counts())\n\n#raggruppo per tweets\ngrouped_df = df_country.groupby('user_screen_name')['tweet'].apply(lambda tweets: ' '.join(tweets)).reset_index()\nprint(f\"Total tweets after concate: {len(grouped_df)}\")\nprint(grouped_df[\"user_screen_name\"].value_counts())\n\n#grouped_followers.head()\n#grouped_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#faccio la join per tenere numero di followers e tweets\ndf_join = pd.merge(grouped_df, grouped_followers, on=\"user_screen_name\", how=\"inner\")\nprint(df_join[\"user_screen_name\"].value_counts())\ndf_join.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calcolo gli hashtags usati per ogni utente\n\n# Funzione per estrarre gli hashtag da un tweet\ndef extract_hashtags(tweet):\n    return re.findall(r'#\\w+', tweet.lower())\n\n# Aggiungere una colonna con gli hashtag estratti\nuser_hashtags = df_join.copy()\nuser_hashtags['hashtags'] = df_join['tweet'].apply(extract_hashtags)\n\n# Trasformo la lista di tweet in un insieme (per non avere duplicati)\nuser_hashtags[\"hashtags\"] = user_hashtags['hashtags'].apply(set)\n\nprint(len(user_hashtags))\nprint(user_hashtags[\"user_screen_name\"].value_counts())\nuser_hashtags.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prendo i top 200\ndf_sorted = user_hashtags.sort_values(by='user_followers_count', ascending=False)\n\ntop_200 = df_sorted.head(200)\n\n#prendo tutti gli utenti con meno di 1000 followers\ndf_less_than = df_sorted[df_sorted[\"user_followers_count\"]<1000]\nprint(df_less_than[\"user_screen_name\"].value_counts())\n\n#campiono 10000 utenti non popolari\nlast_10000 = df_less_than.sample(n=10000, random_state=42) \nprint(last_10000[\"user_screen_name\"].value_counts())\n\n#dataframe uniti\ntotal = pd.concat([top_200,last_10000],axis=0)\nprint(total[\"user_screen_name\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calcolo della similarità tra gli hashtags usando Llama3","metadata":{}},{"cell_type":"code","source":"import subprocess\nimport threading\n\n!pip install langchain-community\n!pip install langchain-core\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh\n    \n#Avvio del server locale di Ollama\nt = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\nt.start()\n\n!ollama pull llama3\n\nt2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\nt2.start()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Valutare il miglior prompt.","metadata":{}},{"cell_type":"code","source":"from langchain_community.llms import Ollama\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\n#Va bene il prompt? Valuta\nprompt = \"You are an hashtags evaluator. Your role is to analyze two groups of hashtags and group classify the two groups as SIMILAR or NOT SIMILAR. You have to answer ONLY with the class (SIMILAR or NOT SIMILAR), not anymore.\"\n\nllm = Ollama(\n    model=\"llama3\"\n)  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", prompt),\n    (\"user\", \"{input}\"),\n])\n\noutput_parser = StrOutputParser()\n\n\ndef hashtags_to_llama(hashtags1,hashtags2):   \n    #chain = template | llm | output_parser\n    \n    #response = chain.invoke({\"input\": \"Tweet 1:\" +tweet1+ \". Tweet 2:\" +tweet2})\n    response = llm.invoke(prompt + \"Hashtags 1:\" + hashtags1 + \". Hashtags 2:\" + hashtags2)\n    \n    return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ora costruisco le 5 partizioni per calcolare le similarità tra gli utenti. Le 5 partizioni sono così costituite: ognuna include 20 nodi top, i quali necessitano il calcolo della similarità con i last_1000. \nServe un'ultimo calcolo interno tra i 100 nodi top.","metadata":{}},{"cell_type":"code","source":"#costruisco partizione su top_100\nimport numpy as np\npartitions = np.array_split(top_100, 5)\n\nfirst_part = partitions[0]\nsecond_part = partitions[1]\nthird_part = partitions[2]\nfourth_part = partitions[3]\nfifth_part = partitions[4]\n\nfirst_part.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"partition = first_part #second_part #third_part #fourth_part #fifth_part\n\ndf = partition.drop(columns=[\"user_followers_count\", \"tweet\"]).merge(last_1000.drop(columns=[\"user_followers_count\", \"tweet\"]), how='cross')\ndf.head()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\n\n# Specifica il nome del file JSON\nfilename = f'/kaggle/working/similarities_{partition}.json'\nrecords = []\n\nfor index, row in tqdm(df.iterrows()):\n    resp = hashtags_to_llama(str(row.hashtags_x),str(row.hashtags_y))\n    #print(resp)\n    record = {\n        \"user1\": row.user_screen_name_x,\n        \"user2\": row.user_screen_name_y,\n        \"similarity\": resp\n    }\n    print(record)\n    records.append(record)\n    \n# Scrivi i dati nel file JSON\nwith open(filename, 'w') as file:\n    json.dump(records, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Costruisco la rete con NetworkX:","metadata":{}},{"cell_type":"code","source":"#Leggo il file json\n\n# Specifica il nome del file JSON\nfilename = '/kaggle/working/similarities.json'\n\n# Carica i dati dal file JSON\ndata = load_json(filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creare un grafo vuoto\nG = nx.Graph()\n\n# Itera su ogni record nel file JSON\nfor item in data:\n    dictionary = dict(item.items())\n    if not G.has_node(dictionary[\"user1\"]): #se utente non presente, lo aggiungo alla rete\n        G.add_node(dictionary[\"user1\"])\n    if not G.has_node(dictionary[\"user2\"]): #se utente non presente, lo aggiungo alla rete\n        G.add_node(dictionary[\"user2\"])\n    if float(dictionary[\"similarity\"])>Threshold: #Oppure devo fare il confronto sulla classificazione (SIMILAR or NOT SIMILAR)\n        G.add_edge(dictionary[\"user1\"], dictionary[\"user2\"], weight=float(dictionary[\"similarity\"]))\n    \n\nprint(f\"Number of nodes: {G.number_of_nodes()}\")\nprint(f\"Number of edges: {G.number_of_edges()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Disegnare il grafo (non si capisce niente, troppi nodi dentro la rete)\n\npos = nx.spring_layout(G)  # Posizionamento dei nodi\nweights = nx.get_edge_attributes(G, 'weight').values()\n\nnx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=5, font_size=5, font_weight='bold')\nnx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{d[\"weight\"]:.2f}' for u, v, d in G.edges(data=True)}, font_color='red')\nnx.draw_networkx_edges(G, pos, width=list(weights))\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot della degree distribution\n\n# Calcolare i gradi dei nodi\ndegrees = [degree for node, degree in G.degree()]\n\n# Calcolare la distribuzione dei gradi\ndegree_count = Counter(degrees)\ndeg, cnt = zip(*degree_count.items())\n\n# Fare il plot della distribuzione dei gradi\nplt.figure(figsize=(8, 6))\nplt.bar(deg, cnt, width=10, color='b')\n\nplt.title(\"Degree Distribution\")\nplt.xlabel(\"Degree\")\nplt.ylabel(\"Frequency\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot della weighted degree\n\n# Calcolare il weighted degree dei nodi\nweighted_degrees = dict(G.degree(weight='weight'))\n\n# Calcolare la distribuzione del weighted degree\nweighted_degree_count = Counter(weighted_degrees.values())\ndeg, cnt = zip(*weighted_degree_count.items())\n\n# Fare il plot della distribuzione del weighted degree\nplt.figure(figsize=(8, 6))\nplt.bar(deg, cnt, width=10, color='b')\n\nplt.title(\"Weighted Degree Distribution\")\nplt.xlabel(\"Weighted Degree\")\nplt.ylabel(\"Frequency\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Betweness Centrality\nbetweenness_centrality = nx.betweenness_centrality(G)\n\n# Ordiniamo i nodi in base ai valori di degree centrality in ordine decrescente\nsorted_degree = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n\n# Stampiamo i nodi con i valori più alti di betweness centrality\nfor node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n    print(f'Nodo: {node}, Betweenness Centrality: {centrality:.6f}')\n\n\n# Degree Centrality\ndegree_centrality = nx.degree_centrality(G)\n\n# Ordiniamo i nodi in base ai valori di degree centrality in ordine decrescente\nsorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n\n# Stampiamo i nodi con i valori più alti di degree centrality\nfor node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n    print(f'Nodo: {node}, Degree Centrality: {centrality:.6f}') \n    \n# Closeness \ncloseness_centrality = nx.closeness_centrality(G)\n\n# Ordiniamo i nodi in base ai valori di degree centrality in ordine decrescente\nsorted_degree = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n\n# Stampiamo i nodi con i valori più alti di degree centrality\nfor node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n    print(f'Nodo: {node}, Closeness Centrality: {centrality:.6f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calcolo del Page Rank\n\npagerank = nx.pagerank(G, alpha=0.85, max_iter=100, tol=1.0e-6)\n\n# Ordiniamo i nodi in base ai valori di page rank\nsorted_degree = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n\n# Stampiamo i nodi con i valori più alti di page rank\nfor node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n    print(f'Nodo: {node}, Page Rank: {centrality:.6f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clustering coefficient\n\n# Global clustering coefficient\nglobal_clustering_coeff = nx.transitivity(G)\nprint(f\"Global clustering coeff: {global_clustering_coeff}\")\n\n# Local clustering coefficient\nlocal_clustering_coeff = nx.average_clustering(G)\nprint(f\"Local clustering coeff: {local_clustering_coeff}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Av. Path Length\n\nprint(nx.average_shortest_path_length(G))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Community detection sulla rete di similarità per scoprire topic comuni","metadata":{}},{"cell_type":"code","source":"import community as community_louvain\n\n# Eseguire la community detection usando l'algoritmo di Louvain\npartition = community_louvain.best_partition(G)\n\n\"\"\"\n# Disegnare il grafo con le comunità\npos = nx.spring_layout(G)\ncmap = plt.get_cmap('viridis')\nnx.draw_networkx_nodes(G, pos, node_size=5, cmap=cmap, node_color=list(partition.values()))\nnx.draw_networkx_edges(G, pos, alpha=0.5)\nplt.show()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"communities = {}\nfor node, community in partition.items():\n    if community not in communities:\n        communities[community] = []\n    communities[community].append(node)\n\n\"\"\"\nfor community, nodes in communities.items():\n    print(f\"Community {community}:\")\n    print(\", \".join(nodes))\n\"\"\"\n\n\n#vedo le community che hanno almeno 10 nodi:\n\n# Filtrare le comunità che hanno almeno 10 nodi\nlarge_communities = {community: nodes for community, nodes in communities.items() if len(nodes) >= 10}\n\n# Stampare il nome dei nodi di ogni comunità con almeno 10 nodi\nfor community, nodes in large_communities.items():\n    print(f\"Community {community} (size: {len(nodes)}):\")\n    print(\", \".join(nodes))\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Studiamo gli hashtags più frequenti per ogni community più numerosa\n\nfor community, nodes in large_communities.items():\n    print(f\"Community: {community}\")\n    print(f\"Num nodes: {len(nodes)}\")\n\n    df_comm = user_hashtags[user_hashtags[\"user_screen_name\"].isin(nodes)]\n    print(df_comm[\"user_screen_name\"].value_counts())\n\n    all_hashtags = [hashtag for hashtags in df_comm['hashtags'] for hashtag in hashtags]\n\n    hashtag_counts = Counter(all_hashtags)\n\n    sorted_hashtag_counts = hashtag_counts.most_common()\n\n    # Stampare la classifica degli hashtag\n    print(\"Classifica degli hashtag più usati:\")\n    for hashtag, count in sorted_hashtag_counts[:50]:\n        print(f\"{hashtag}: {count}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef preprocess_text(text):\n    # Rimozione di URL, menzioni e hashtag\n    text = re.sub(r\"http\\S+|@\\S+|#\\S+\", \"\", text)\n    # Rimozione di punteggiatura e numeri\n    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n    # Convertire il testo in minuscolo\n    text = text.lower()\n    # Tokenizzazione\n    tokens = word_tokenize(text)\n    # Rimozione delle stopword\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\n    return tokens\n\n# Analizzare le comunità per determinare i topic\nfor community, nodes in large_communities.items():\n    all_words = []\n    for node in nodes:\n        all_words.extend(preprocess_text(user_hashtags.loc[user_hashtags[\"user_screen_name\"]==node, 'tweet'].values[0]))\n    word_counts = Counter(all_words)\n    most_common_words = word_counts.most_common(100)\n    print(f\"Community {community} (size: {len(nodes)}):\")\n    print(\"Most common words:\", most_common_words)\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Uso di BERTopic per scoprire i Topic all'interno delle community","metadata":{}},{"cell_type":"markdown","source":"Devo prima addestrare il modello a trovare i topic su tutti i tweet, e poi fare inferenza dei topic sulle singole community.","metadata":{}},{"cell_type":"code","source":"!pip install bertopic\n\nfrom bertopic import BERTopic\n\nmodel = BERTopic() #serve\n\n#topic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\") #carica i topic già presenti da Wikipedia","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topics, probs = model.fit_transform(list(df_country[\"tweet\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/bertopic')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTopic.load('/kaggle/working/bertopic')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prendo i tweet degli utenti che appartengono alla community\ncommunity = 0 #indice della community\ndf = df_join[df_join[\"user_screen_name\"].isin(dict(large_communities.items())[community])]\ntweets = df[\"tweet\"]\n\nprint(tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.visualize_topics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topics, probs = model.transform(list(tweets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Occorre un training sui topics?","metadata":{}},{"cell_type":"code","source":"# Effettuo un campionamento casuale del dataset (gli utenti sono troppi e non riusciremmo a costruire la rete)\n\n\"\"\"\ndf_sampled = grouped_conc.sample(frac=0.2, random_state=42)\nprint(df_sampled[\"user_screen_name\"].value_counts())\n\ndf_sampled.head()\n# Idea di altro campionamento: \n# stimo i degree in modo parallelo (calcolo similarità dei primi 100 utenti con tutti gli altri)\n# campiono seguendo la stima della distribuzione\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calcolo classico della similarità con Jaccard non va bene. Quindi commento il codice:","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThreshold = 0.3\n\n# Funzione per calcolare la similarità di Jaccard\ndef jaccard_similarity(set1, set2):\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    if union == 0:\n        return 0\n    return intersection / union\n\ndf_final = user_hashtags.drop(columns=[\"tweet\"])\n# Calcolare la similarità di Jaccard tra ogni coppia di utenti\nedges = []\nfor (user1, hashtags1), (user2, hashtags2) in combinations(user_hashtags.drop(columns=[\"tweet\"]).itertuples(index=False), 2):\n    similarity = jaccard_similarity(hashtags1, hashtags2)\n    if similarity > Threshold:  # Aggiungere solo archi con similarità positiva\n        edges.append((user1, user2, similarity))\n\n# Creare un grafo vuoto\nG = nx.Graph()\n\n# Aggiungere nodi (utenti)\nfor user in df_final['user_screen_name']: \n    G.add_node(user)\n\n# Aggiungere archi con pesi (similarità di Jaccard)\nfor user1, user2, weight in edges:\n    G.add_edge(user1, user2, weight=weight)\n    \n\nprint(f\"Number of nodes: {G.number_of_nodes()}\")\nprint(f\"Number of edges: {G.number_of_edges()}\")\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#Prendiamo gli utenti che hanno degree 998.\n\ndesired_degree = 767 #950, #767, #998\n\n# Filtrare i nodi che hanno il grado specificato\nnodes_with_desired_degree = [node for node, degree in degree_dict.items() if degree == desired_degree]\n\ndf_giant = df_final[df_final[\"user_screen_name\"].isin(nodes_with_desired_degree)]\n# Stampare i nodi con il grado desiderato\nprint(f\"Nodi con grado {df_giant}:\")\nprint(df_giant)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Si potrebbe effettuare un campionamento dei nodi tenendo conto della degree distribution dei nodi","metadata":{}},{"cell_type":"code","source":"# Definire la funzione di campionamento basato sui gradi\n\"\"\"\ndef degree_based_sampling(graph, sample_size):\n    # Calcolare i gradi dei nodi\n    degrees = dict(graph.degree())\n    nodes, degree_values = zip(*degrees.items())\n    \n    # Convertire i gradi in probabilità (più alto il grado, maggiore la probabilità di essere selezionato)\n    total_degree = sum(degree_values)\n    probabilities = [degree / total_degree for degree in degree_values]\n    \n    # Campionare i nodi in base alle probabilità\n    sampled_nodes = np.random.choice(nodes, size=sample_size, replace=False, p=probabilities)\n    \n    # Restituire il sottografo campionato\n    return graph.subgraph(sampled_nodes)\n\n# Campionare il 20% dei nodi basato sui gradi\nsample_size = int(len(G.nodes) * 0.2)\nG_sampled = degree_based_sampling(G, sample_size)\n\n# Calcolare la distribuzione dei gradi nel grafo campionato\nsampled_degrees = [degree for node, degree in G_sampled.degree()]\nsampled_degree_count = Counter(sampled_degrees)\nsampled_deg, sampled_cnt = zip(*sampled_degree_count.items())\n\n# Fare il plot della distribuzione dei gradi nel grafo campionato\nplt.figure(figsize=(8, 6))\nplt.bar(sampled_deg, sampled_cnt, width=0.80, color='b')\n\nplt.title(\"Degree Distribution in Sampled Graph\")\nplt.xlabel(\"Degree\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Fare il plot della distribuzione dei gradi nel grafo originale per confronto\noriginal_degrees = [degree for node, degree in G.degree()]\noriginal_degree_count = Counter(original_degrees)\norig_deg, orig_cnt = zip(*original_degree_count.items())\n\nplt.figure(figsize=(8, 6))\nplt.bar(orig_deg, orig_cnt, width=0.80, color='r')\n\nplt.title(\"Degree Distribution in Original Graph\")\nplt.xlabel(\"Degree\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Costruisco la rete con similarità usando Sentence Bert","metadata":{}},{"cell_type":"markdown","source":"Uso i 200 utenti più popolari (in termini di followers) e campiono 10000 utenti non popolari (sotto i 1000 followers).","metadata":{}},{"cell_type":"markdown","source":"Utilizzo pipeline transformers per filtrare tutti i tweet che non sono in inglese. (Non funziona correttamente!)","metadata":{}},{"cell_type":"code","source":"\"\"\"\nfrom transformers import pipeline\nimport torch\nfrom tqdm import tqdm\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nclassifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n\nfilename=\"/kaggle/working/user_to_filter.json\"\nuser_to_filter = []\n\nfor row in tqdm(df_sampled.itertuples(index=True, name='Pandas')):\n    candidate_labels = ['english language', 'not english language']\n    resp = classifier(row.tweet, candidate_labels)[\"labels\"][0]\n    print(row.tweet)\n    print(resp)\n    if resp == \"not english language\":\n        record = {\n            \"user\": row.user_screen_name\n        }\n        user_to_filter.append(record)\n\n    \n# Scrivi i dati nel file JSON\nwith open(filename, 'w') as file:\n    json.dump(user_to_filter, file)\n\n#crea nuovo df leggendo json con utenti da eliminare\n\n# Funzione per caricare il contenuto di un file JSON\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\n# Specifica il nome del file JSON\nfilename=\"/kaggle/working/user_to_filter.json\"\n\n# Carica i dati dal file JSON\ndata = load_json(filename)\nuser_to_filter = []\n\n# Itera su ogni record nel file JSON\nfor item in data:\n    dictionary = dict(item.items())\n    user_to_filter.append(dictionary[\"user\"])\n\n# Elimino da grouped_df gli utenti che non hanno tweet in inglese\nindexes = grouped_df[gouped_df['user_screen_name'].isin(user_to_filter)].index\n\n# Eliminare le righe usando il metodo drop\ndf_filtered = grouped_df.drop(indexes)\n\nprint(f\"Users before filter: {len(grouped_df)}\")\nprint(f\"Users after filter: {len(df_filtered)}\")\n\n\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summarization con t5","metadata":{}},{"cell_type":"markdown","source":"Utilizzo pipeline per la summarization per testi troppo lunghi. Problematica, alcuni testi sono eccessivamente lunghi e il modello va out of memory. Soluzione: tronco l'input.","metadata":{}},{"cell_type":"code","source":"total.head()\nlen(total)\n\n# Lunghezza media dei tweet\nlen_tweets = 0\nlist_len = []\nfor row in total.itertuples(index=True, name='Pandas'):\n    #print(row)\n    len_tweets = len_tweets + len(row.tweet)\n    list_len.append(len(row.tweet))\n\nlist_len.sort()\nprint(f\"lunghezza media dei tweet:{(len_tweets/len(total)):.2f}\")\nprint(f\"50° percentile:{list_len[int(len(total)*0.5)]}\")\nprint(f\"80° percentile:{list_len[int(len(total)*0.7)]}\")\nprint(f\"95° percentile:{list_len[int(len(total)*0.95)]}\")\n\n\n#Osservazione, la frequenza di pubblicazione si distribuisce secondo una power-law?\n# Calcolare la distribuzione dei gradi\ndegree_count = Counter(list_len)\nprint(degree_count)\ndeg, cnt = zip(*degree_count.items())\n\n# Fare il plot della distribuzione dei gradi\nplt.figure(figsize=(8, 6))\nplt.bar(deg, cnt, width=200, color='b')\n\nplt.title(\"Publishing distribution\")\nplt.xlabel(\"Lunghezza tweet\")\nplt.ylabel(\"Users\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verifica con plot log-log\n\n\n# Creare un grafico\nplt.figure()\n\n# Tracciare i dati\nplt.plot(deg, cnt, label='Publishing distribution')\n\n# Impostare la scala logaritmica sugli assi x e y\nplt.xscale('log')\nplt.yscale('log')\n\n# Aggiungere etichette e titolo\nplt.xlabel('Lunghezza tweet')\nplt.ylabel('Users')\nplt.title('Grafico con assi in scala logaritmica')\nplt.legend()\n\n# Mostrare il grafico\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\nfrom tqdm import tqdm\nimport json\n\ndevice = 0 if torch.cuda.is_available() else -1\n\nsummarizer = pipeline(task=\"summarization\", model=\"google-t5/t5-base\", tokenizer=\"google-t5/t5-base\", device=device)\n\n#3000 non va bene, probabilmente occorre abbassarla ulteriormente\nThreshold = 3000 #soglia sul numero di caratteri, se viene superata questa soglia, il testo viene riassunto\n\nfilename=\"/kaggle/working/summarization.json\"\nsummarized = []\n\nfor row in tqdm(total.itertuples(index=True, name='Pandas')): #df_filtered\n    if (len(row.tweet)>Threshold):\n        text = row.tweet\n        if (len(row.tweet)>10000): #se il testo è oltre i 10.000 caratteri, lo tronco\n            text = text[:10000]\n        #print(text)\n        resp = summarizer(text)\n        #print(resp)\n        record = {\n            \"user\": row.user_screen_name,\n            \"summerized\": resp[0][\"summary_text\"]\n        }\n        summarized.append(record)\n\n    \n# Scrivi i dati nel file JSON\nwith open(filename, 'w') as file:\n    json.dump(summarized, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#controllo per vedere se sono rimasti tweet con più di 3000 caratteri    \nfor row in total.itertuples(index=True, name='Pandas'):\n    if (len(row.tweet)>Threshold):\n        print(\"Tweet con più di 3000 caratteri\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Codice per sostituire i tweet con i riassunti\nimport json\nThreshold=3000\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\n# Specifica il nome del file JSON\nfilename=\"/kaggle/working/summarization.json\"\n\n# Carica i dati dal file JSON\ndata = load_json(filename)\n\n# Itera su ogni record nel file JSON\nfor item in data:\n    dictionary = dict(item.items())\n    total.loc[total['user_screen_name'] == dictionary[\"user\"], 'tweet'] = dictionary[\"summerized\"]\n    top_200.loc[top_200['user_screen_name'] == dictionary[\"user\"], 'tweet'] = dictionary[\"summerized\"]\n    last_10000.loc[last_10000['user_screen_name'] == dictionary[\"user\"], 'tweet'] = dictionary[\"summerized\"]\n\n#controllo per vedere se sono rimasti tweet con più di 3000 caratteri    \nfor row in total.itertuples(index=True, name='Pandas'):\n    if (len(row.tweet)>Threshold):\n        print(\"Tweet con più di 3000 caratteri\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ora costruisco le 5 partizioni per calcolare le similarità tra gli utenti. Le 5 partizioni sono così costituite: ognuna include 20 nodi top, i quali necessitano il calcolo della similarità con i last_1000. \nServe un'ultimo calcolo interno tra i 100 nodi top.\nIn questo caso non serve perché SBERT è abbastanza veloce.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n#costruisco partizione su top_100\nimport numpy as np\npartitions = np.array_split(top_100, 5)\n\nfirst_part = partitions[0]\nsecond_part = partitions[1]\nthird_part = partitions[2]\nfourth_part = partitions[3]\nfifth_part = partitions[4]\n\nfirst_part.head()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport itertools\nlista = list(itertools.product(first_part.drop(columns=[\"user_followers_count\"]).iterrows(), last_1000.drop(columns=[\"user_followers_count\"]).iterrows()))\nfor i in lista:\n    print(i[0][1])\n    print(i[1])\n    break\n\"\"\"\n\n#partition = first_part #second_part #third_part #fourth_part #fifth_part\n\n# Costruzione di tutte le combinazioni\ndf = top_200.drop(columns=[\"user_followers_count\",\"hashtags\"]).merge(last_10000.drop(columns=[\"user_followers_count\", \"hashtags\"]), how='cross')\ndf.head()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SentenceBert","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport json\nfrom tqdm import tqdm\n\n#device = 0 if torch.cuda.is_available() else -1\n\n# 1. Load a pretrained Sentence Transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\") \n\n# Specifica il nome del file JSON\nfilename = f'/kaggle/working/similarities_sbert.json'\nrecords = []\n\n\"\"\"\nfor index, row in tqdm(df.iterrows()):\n    embeddings = model.encode([row.tweet_x,row.tweet_y])\n    #print(embeddings.shape)\n    similarities = model.similarity(embeddings, embeddings)\n    #print(similarities)\n    resp = round(similarities[0][1].item(),2)\n    record = {\n        \"user1\": row.user_screen_name_x,\n        \"user2\": row.user_screen_name_y,\n        \"similarity\": resp\n    }\n    #print(record)\n    records.append(record)\n\"\"\"\n\n# Provo a passare una matrice di tweet\nfor index, row in tqdm(top_200.iterrows()):\n    popular = row.to_frame().T\n    couples = popular.drop(columns=[\"user_followers_count\", \"hashtags\"]).merge(last_10000.drop(columns=[\"user_followers_count\", \"hashtags\"]), how='cross')\n    lista = [str(row.tweet)] + last_10000[\"tweet\"].tolist()\n    #print(len(lista))\n    embeddings = model.encode(lista)\n    similarities = model.similarity(embeddings, embeddings)\n    first = True\n    for index, row in couples.iterrows():\n        if first: \n            first = False\n            pass\n        else:\n            resp = round(similarities[0][index].item(),2)\n            record = {\n                \"user1\": row.user_screen_name_x,\n                \"user2\": row.user_screen_name_y,\n                \"similarity\": resp\n            }\n            records.append(record)\n\n            \n\n# Scrivi i dati nel file JSON\nwith open(filename, 'w') as file:\n    json.dump(records, file, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T16:09:59.061655Z","iopub.execute_input":"2024-07-17T16:09:59.062031Z","iopub.status.idle":"2024-07-17T16:42:15.208965Z","shell.execute_reply.started":"2024-07-17T16:09:59.062001Z","shell.execute_reply":"2024-07-17T16:42:15.208104Z"},"trusted":true},"execution_count":253,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8b9ca297d774b019bd920c18b367452"}},"metadata":{}},{"name":"stderr","text":"1it [00:09,  9.85s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d991929443f4582a30b5c3b28e4479b"}},"metadata":{}},{"name":"stderr","text":"2it [00:19,  9.63s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"752bda9704de4bf9a976485a5da49253"}},"metadata":{}},{"name":"stderr","text":"3it [00:28,  9.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e846776323e47beae08a3c2ac30c047"}},"metadata":{}},{"name":"stderr","text":"4it [00:38,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8486a3f8edc44dd69ba0b016c8861044"}},"metadata":{}},{"name":"stderr","text":"5it [00:47,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf1a79cdde6473ea66d31397df637e6"}},"metadata":{}},{"name":"stderr","text":"6it [00:57,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1a1db926824e8db9e060f70a9a96c8"}},"metadata":{}},{"name":"stderr","text":"7it [01:06,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f910d3d833d4be1b983b22115ed8959"}},"metadata":{}},{"name":"stderr","text":"8it [01:16,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f4f759108f04a149619e6a73c9ea060"}},"metadata":{}},{"name":"stderr","text":"9it [01:25,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ac9903c64740dca8291fa2b9406b55"}},"metadata":{}},{"name":"stderr","text":"10it [01:35,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448eb57d827048faa4c854f6c4512ffa"}},"metadata":{}},{"name":"stderr","text":"11it [01:44,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5162b4f02f6a429eb91075014cc4e74c"}},"metadata":{}},{"name":"stderr","text":"12it [01:54,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2534545f393c4b73862db1e791318e54"}},"metadata":{}},{"name":"stderr","text":"Exception ignored in: <function tqdm.__del__ at 0x7ba2c8033d90>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/tqdm/std.py\", line 1148, in __del__\n    self.close()\n  File \"/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py\", line 279, in close\n    self.disp(bar_style='danger', check_delay=False)\nAttributeError: 'tqdm_notebook' object has no attribute 'disp'\n13it [02:17, 13.60s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6882e15055438e86bc6922b7051be1"}},"metadata":{}},{"name":"stderr","text":"14it [02:26, 12.35s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1474a024ccc0403f8839e6b255ce849d"}},"metadata":{}},{"name":"stderr","text":"15it [02:36, 11.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f232f3d89fdb4ef99cc30065a60bdc57"}},"metadata":{}},{"name":"stderr","text":"16it [02:45, 10.90s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e0be9ee235488593f8596e691e2f5d"}},"metadata":{}},{"name":"stderr","text":"17it [02:55, 10.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bef1aa385784c19806fc46cf04fe638"}},"metadata":{}},{"name":"stderr","text":"18it [03:04, 10.22s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e839d5b2cc2452891ed4e36f7965096"}},"metadata":{}},{"name":"stderr","text":"19it [03:14, 10.00s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0e74f9f51344dbb86f640c0536afdf"}},"metadata":{}},{"name":"stderr","text":"20it [03:23,  9.83s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f360628ede46beabbe864ee47ba302"}},"metadata":{}},{"name":"stderr","text":"21it [03:33,  9.72s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba2f4f46ce34cefad5f4814ae632cc3"}},"metadata":{}},{"name":"stderr","text":"22it [03:42,  9.66s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef5c320f86043ba9b7f656a96ba82ec"}},"metadata":{}},{"name":"stderr","text":"23it [03:52,  9.60s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0203fa8c3447338b4239c5794b10bd"}},"metadata":{}},{"name":"stderr","text":"24it [04:01,  9.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"404242f198dd4cd2ada801cf3b85d900"}},"metadata":{}},{"name":"stderr","text":"25it [04:11,  9.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f3553bfb544a34a60ea55f3a570894"}},"metadata":{}},{"name":"stderr","text":"26it [04:20,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2eaaaa142c74e5da662cb1fa24875d1"}},"metadata":{}},{"name":"stderr","text":"27it [04:30,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d12962c00a4a909826e5f834b41abf"}},"metadata":{}},{"name":"stderr","text":"28it [04:39,  9.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d92a86c266e24ce58696854a8600509a"}},"metadata":{}},{"name":"stderr","text":"29it [04:49,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4133cd1b604c1c920dc2a018d8b5e8"}},"metadata":{}},{"name":"stderr","text":"30it [04:58,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27635ee7f49a4235a8ff9e2084b90511"}},"metadata":{}},{"name":"stderr","text":"31it [05:08,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f99966f04fa4d629a7f0268685e8a63"}},"metadata":{}},{"name":"stderr","text":"32it [05:17,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d066616b21497590ca88196716ad3d"}},"metadata":{}},{"name":"stderr","text":"33it [05:27,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5708a3ea92884411b945b96a0b407c53"}},"metadata":{}},{"name":"stderr","text":"34it [05:36,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd725e900ed342fb9451ab4553088171"}},"metadata":{}},{"name":"stderr","text":"35it [05:46,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85908b35f60a443480c262ba3001fc1d"}},"metadata":{}},{"name":"stderr","text":"36it [05:55,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d41d38060be4fbf8ab5cfe2f79cb8ca"}},"metadata":{}},{"name":"stderr","text":"37it [06:05,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d2bbb44f33d4495819ecaaf6319089a"}},"metadata":{}},{"name":"stderr","text":"38it [06:14,  9.55s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab0da500838d4676aeb95573c89f0d2f"}},"metadata":{}},{"name":"stderr","text":"39it [06:24,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2a4629b32e24699bcb4e57bbb1ba6a2"}},"metadata":{}},{"name":"stderr","text":"40it [06:33,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee463974c844de9bde7df655cf80a0b"}},"metadata":{}},{"name":"stderr","text":"41it [06:43,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f391f5a229a41d3acac42edc0acbb51"}},"metadata":{}},{"name":"stderr","text":"42it [06:52,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176d77b78e8a44c085af5413a1e1ddd8"}},"metadata":{}},{"name":"stderr","text":"43it [07:02,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df3969ea339c44cfb1e2418078938d2d"}},"metadata":{}},{"name":"stderr","text":"44it [07:11,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f065f7c8c6a4ea1943f6349c84daf1c"}},"metadata":{}},{"name":"stderr","text":"45it [07:21,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc4b851970a4e8d87fabb6796182c87"}},"metadata":{}},{"name":"stderr","text":"46it [07:30,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9429402755d545d58790f70f8aec5f89"}},"metadata":{}},{"name":"stderr","text":"47it [07:40,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4eae7904794d5a80d8afa67a1b1f1f"}},"metadata":{}},{"name":"stderr","text":"48it [07:49,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2511f7b7119401aae48eac1f1257673"}},"metadata":{}},{"name":"stderr","text":"49it [07:59,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c102814c8643c8a89ddee1ff92b3ad"}},"metadata":{}},{"name":"stderr","text":"50it [08:08,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"484aa58735464886950f24755b8a27d1"}},"metadata":{}},{"name":"stderr","text":"51it [08:18,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d63df24f034d2dadfb56f159fb9238"}},"metadata":{}},{"name":"stderr","text":"52it [08:27,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7e65866f8f6412eb1e2d23e7734dc24"}},"metadata":{}},{"name":"stderr","text":"53it [08:37,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e77cced406e2433b994eaee5c5383f58"}},"metadata":{}},{"name":"stderr","text":"54it [08:46,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594d8583b5c04c7fa11ef29740e09b7e"}},"metadata":{}},{"name":"stderr","text":"55it [08:56,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e3bfb07976949d7bebe23a50d1a6d0b"}},"metadata":{}},{"name":"stderr","text":"56it [09:05,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c7b738eec24d4c98516e04eac0fbc3"}},"metadata":{}},{"name":"stderr","text":"57it [09:15,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba3bc8e4ae0440dba5c685310e27c629"}},"metadata":{}},{"name":"stderr","text":"58it [09:24,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d8571b688946c1ab081e48332b66fe"}},"metadata":{}},{"name":"stderr","text":"59it [09:34,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15144d904074a4689d114fd6504e30b"}},"metadata":{}},{"name":"stderr","text":"60it [09:43,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d99bfa9eb340e9a3215461da8f427a"}},"metadata":{}},{"name":"stderr","text":"61it [09:53,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87e3ec92ce0c4ad19e40afe2a8fd17d2"}},"metadata":{}},{"name":"stderr","text":"62it [10:02,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1055b7f710c6459b8960d8a88d00ca50"}},"metadata":{}},{"name":"stderr","text":"63it [10:12,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1de3cae8294669a86b4b920ab5d1fb"}},"metadata":{}},{"name":"stderr","text":"64it [10:21,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98095c864c0948ecaaff058975d314e8"}},"metadata":{}},{"name":"stderr","text":"65it [10:31,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f444a8771d45468728bf69dd5efb40"}},"metadata":{}},{"name":"stderr","text":"66it [10:40,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbfc6f215a2402db8e0a408b703a10f"}},"metadata":{}},{"name":"stderr","text":"67it [10:50,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b069cded4248229424289d2a29ff46"}},"metadata":{}},{"name":"stderr","text":"68it [10:59,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e48d88ecde84d6fbded6a7e9f8dbc7b"}},"metadata":{}},{"name":"stderr","text":"69it [11:09,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaa7c4fdc4e64960a8a2fc6b147d8f80"}},"metadata":{}},{"name":"stderr","text":"70it [11:18,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a68695ec6b415ab5ff717715facec9"}},"metadata":{}},{"name":"stderr","text":"71it [11:28,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108ebeb0af6d4da99ee86c93c35cb1b7"}},"metadata":{}},{"name":"stderr","text":"72it [11:37,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e50bc41f21c41798c6bae63ea7aafee"}},"metadata":{}},{"name":"stderr","text":"73it [11:47,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2231b0d662b446299f3088790350e7f"}},"metadata":{}},{"name":"stderr","text":"74it [11:56,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf9865f7ba144fe9a22836f6744ea48"}},"metadata":{}},{"name":"stderr","text":"75it [12:06,  9.55s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e96536662c14c98a136a9c93825037d"}},"metadata":{}},{"name":"stderr","text":"76it [12:15,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c5f469b63346deabcb08d4ad76a15a"}},"metadata":{}},{"name":"stderr","text":"77it [12:25,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f449daad68e54aed98eb1d3ae86feb81"}},"metadata":{}},{"name":"stderr","text":"78it [12:34,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d615869ff5e4dee8dd4d7c3298bc026"}},"metadata":{}},{"name":"stderr","text":"79it [12:44,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b5c6be8dfb5426ead07440c0f8c21c3"}},"metadata":{}},{"name":"stderr","text":"80it [12:53,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df55948b873d406b8328ab2e7091607f"}},"metadata":{}},{"name":"stderr","text":"81it [13:03,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ab27949e3f400c98158f6b289fd56c"}},"metadata":{}},{"name":"stderr","text":"82it [13:12,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6443f0ef3e49d29f7bf29abac892f2"}},"metadata":{}},{"name":"stderr","text":"83it [13:22,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5904750b4e42e097aa489f8ef42231"}},"metadata":{}},{"name":"stderr","text":"84it [13:31,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3458ccff8b4943a6ed9dac7ce45725"}},"metadata":{}},{"name":"stderr","text":"85it [13:41,  9.53s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"510bb595fb4c416ab7d2a2b17a8efb5a"}},"metadata":{}},{"name":"stderr","text":"86it [13:50,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f1ca16d0474d308a69a820480f3e76"}},"metadata":{}},{"name":"stderr","text":"87it [14:00,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a13b91824054c5aac1c628250b8b4c5"}},"metadata":{}},{"name":"stderr","text":"88it [14:09,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c96e6ab5dea419db3b3c49a56df7c28"}},"metadata":{}},{"name":"stderr","text":"89it [14:19,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c3cbd3d167e4991a63aeaf9c5f5c7f0"}},"metadata":{}},{"name":"stderr","text":"90it [14:28,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b196edc1b606407d9b19cf400efb210b"}},"metadata":{}},{"name":"stderr","text":"91it [14:38,  9.45s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e9af62daaa4570bf1e063fbf48a17a"}},"metadata":{}},{"name":"stderr","text":"92it [14:47,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d28524a18c94297a39a9fefea177da8"}},"metadata":{}},{"name":"stderr","text":"93it [14:56,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c366b71e3d2481f9089b2ac489f05d1"}},"metadata":{}},{"name":"stderr","text":"94it [15:06,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b42be9fb1c47b2885572448d09fe20"}},"metadata":{}},{"name":"stderr","text":"95it [15:15,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea998507ed14289882b681f83abdddf"}},"metadata":{}},{"name":"stderr","text":"96it [15:25,  9.42s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa9f10d9f8e64a1d980e8b457bb5b576"}},"metadata":{}},{"name":"stderr","text":"97it [15:34,  9.42s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c17e9c490374296b127319998bc8bdd"}},"metadata":{}},{"name":"stderr","text":"98it [15:44,  9.42s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a712e7c795f4374b7e5ddaee0a1137e"}},"metadata":{}},{"name":"stderr","text":"99it [15:53,  9.42s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"759e21384beb4b7aaf94999a1a91d442"}},"metadata":{}},{"name":"stderr","text":"100it [16:02,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46fb7576b8c04cc9b59d5d420e03976e"}},"metadata":{}},{"name":"stderr","text":"101it [16:12,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8a875770e094e9182458c025e84e610"}},"metadata":{}},{"name":"stderr","text":"102it [16:21,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccb877fc10694d52b30edf24a4068dad"}},"metadata":{}},{"name":"stderr","text":"103it [16:31,  9.45s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64c5ab95670741ff80545b4fad4c6f92"}},"metadata":{}},{"name":"stderr","text":"104it [16:40,  9.45s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e94e900654d64bdb9549dd44b7b9ed0e"}},"metadata":{}},{"name":"stderr","text":"105it [16:50,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcb07899c17e42c09fcb7c8a67bf03e3"}},"metadata":{}},{"name":"stderr","text":"106it [16:59,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55c113e63a4e45d69f3d61da008a9e2c"}},"metadata":{}},{"name":"stderr","text":"107it [17:08,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75ef3aaf2af44278ab713b5cc67dfc11"}},"metadata":{}},{"name":"stderr","text":"108it [17:18,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d9feb9de6f4576a3f6552520013a86"}},"metadata":{}},{"name":"stderr","text":"109it [17:27,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81aa9b41e6d6429b8f8bd07622821752"}},"metadata":{}},{"name":"stderr","text":"110it [17:37,  9.42s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ee2da708a7c496b8475d23b07709abf"}},"metadata":{}},{"name":"stderr","text":"111it [17:46,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0f3375205b0475583c4bd5abf14d3db"}},"metadata":{}},{"name":"stderr","text":"112it [17:56,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b0fa0126574c6ca9d3fdc621993373"}},"metadata":{}},{"name":"stderr","text":"113it [18:05,  9.43s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f1ce37350e49ec865b380fb703d3f3"}},"metadata":{}},{"name":"stderr","text":"114it [18:14,  9.42s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2477d6f3720d461d8610ac860856c340"}},"metadata":{}},{"name":"stderr","text":"115it [18:24,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57365958a33b488a997d85d0fe4e84f4"}},"metadata":{}},{"name":"stderr","text":"116it [18:33,  9.45s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"007e9fcdc06644db8011b921f653531c"}},"metadata":{}},{"name":"stderr","text":"117it [18:43,  9.45s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc237ef6812e4f19a79d54f55f09cc6f"}},"metadata":{}},{"name":"stderr","text":"118it [18:52,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65a5ac2c500f43f4944fe498b4cff73a"}},"metadata":{}},{"name":"stderr","text":"119it [19:02,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10859016aa9c41d996938be006d9d47b"}},"metadata":{}},{"name":"stderr","text":"120it [19:11,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2431bae955ce4311af1c4413433de5c7"}},"metadata":{}},{"name":"stderr","text":"121it [19:21,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2945c3a231a4fc4abc0d14b83f118cd"}},"metadata":{}},{"name":"stderr","text":"122it [19:30,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c447e74955144f7e869cddd839674509"}},"metadata":{}},{"name":"stderr","text":"123it [19:40,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e49c21867ee4bd1a5a09d3e0e432802"}},"metadata":{}},{"name":"stderr","text":"124it [19:49,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce33a6a3c65c4730abbd535789c3a607"}},"metadata":{}},{"name":"stderr","text":"125it [19:59,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e980de151064c55a089b63e92c13024"}},"metadata":{}},{"name":"stderr","text":"126it [20:08,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2d3144891549b689bc6afe60e6a11a"}},"metadata":{}},{"name":"stderr","text":"127it [20:18,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5c86c529fe4cf5bb36a6179b1537f3"}},"metadata":{}},{"name":"stderr","text":"128it [20:27,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ecb86d69ec420abf1f5c3c63ab88e5"}},"metadata":{}},{"name":"stderr","text":"129it [20:37,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949d571e5f4a48318453138e52c533d1"}},"metadata":{}},{"name":"stderr","text":"130it [20:46,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74bb0872d0d54046b0d4c41245c428aa"}},"metadata":{}},{"name":"stderr","text":"131it [20:56,  9.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff74e48a3ff94cd89b4826d8c2d05211"}},"metadata":{}},{"name":"stderr","text":"132it [21:05,  9.53s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe31848422944cda9d5fd4f59715b9e"}},"metadata":{}},{"name":"stderr","text":"133it [21:15,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02137601daf74b0a9d8da4e718fff9ca"}},"metadata":{}},{"name":"stderr","text":"134it [21:24,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0daad3d68b41979ffb2a6f9c137614"}},"metadata":{}},{"name":"stderr","text":"135it [21:34,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da10aec0ac6b489aa3ea893b6ac4b789"}},"metadata":{}},{"name":"stderr","text":"136it [21:43,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afbff683c9404d76869b40eefdf998c0"}},"metadata":{}},{"name":"stderr","text":"137it [21:53,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d159a6955e45c985644df8d33b6419"}},"metadata":{}},{"name":"stderr","text":"138it [22:02,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d54c5160b34414bb70ad15e010cb89f"}},"metadata":{}},{"name":"stderr","text":"139it [22:12,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d626cf9079ae4bb6ad3da6f29e2ddfb2"}},"metadata":{}},{"name":"stderr","text":"140it [22:21,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689668c4691e486f91596ab0c3d229d8"}},"metadata":{}},{"name":"stderr","text":"141it [22:31,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"887e4d84aae24cb0adde927928d5a534"}},"metadata":{}},{"name":"stderr","text":"142it [22:40,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2df12c3038b84371882c0596eb0df8e5"}},"metadata":{}},{"name":"stderr","text":"143it [22:50,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b3a6b4a3c3047a180221fbded98747e"}},"metadata":{}},{"name":"stderr","text":"144it [22:59,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"652ca84a197e4cdba0125b28148f09c2"}},"metadata":{}},{"name":"stderr","text":"145it [23:09,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bc98c00fb504172b3af03da940a0841"}},"metadata":{}},{"name":"stderr","text":"146it [23:18,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc439484cebb42b1b1a5c230261fd1bc"}},"metadata":{}},{"name":"stderr","text":"147it [23:28,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd43f3e221ca45bc9345bd0c057258e3"}},"metadata":{}},{"name":"stderr","text":"148it [23:37,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11811d66f78e4435ba32108a95a9f8f7"}},"metadata":{}},{"name":"stderr","text":"149it [23:47,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6953016b78ea4548bb0554f7d5e41382"}},"metadata":{}},{"name":"stderr","text":"150it [23:56,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18aaa2b28f843d5a1961a758c7aa1cc"}},"metadata":{}},{"name":"stderr","text":"151it [24:06,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4de996ed6575490d85f0f933b58db0cb"}},"metadata":{}},{"name":"stderr","text":"152it [24:15,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb6db8cbd6244439f7bdf57e0a4af64"}},"metadata":{}},{"name":"stderr","text":"153it [24:24,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"172fe522d180442081c658683a08d829"}},"metadata":{}},{"name":"stderr","text":"154it [24:34,  9.45s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a93e6f0f13f4b49950a1707589d14d0"}},"metadata":{}},{"name":"stderr","text":"155it [24:43,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5442956d7994446bf7f7941d7e60609"}},"metadata":{}},{"name":"stderr","text":"156it [24:53,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e1b2304e7e443cfb372ed15aaac39c6"}},"metadata":{}},{"name":"stderr","text":"157it [25:02,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0009b1086f5543d4844e10efb65a88a3"}},"metadata":{}},{"name":"stderr","text":"158it [25:12,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10f962402d74465a1575ed3971bd1d4"}},"metadata":{}},{"name":"stderr","text":"159it [25:21,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e699892d92248989aee34a22f753f72"}},"metadata":{}},{"name":"stderr","text":"160it [25:31,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43ce6919ee214c14ada8728937baac0b"}},"metadata":{}},{"name":"stderr","text":"161it [25:40,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003cb597a0c8492aa26c864bebf38405"}},"metadata":{}},{"name":"stderr","text":"162it [25:50,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c5e735113445958e4da733531cd7b0"}},"metadata":{}},{"name":"stderr","text":"163it [25:59,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc6b246985ab4501afdb8c367cc0ddf8"}},"metadata":{}},{"name":"stderr","text":"164it [26:09,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a1611165ac4b66ac656c4814d20c09"}},"metadata":{}},{"name":"stderr","text":"165it [26:18,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6181773854f4442d909a25db0cd23e7c"}},"metadata":{}},{"name":"stderr","text":"166it [26:28,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1923880c0284a0db7d0c7a28884d572"}},"metadata":{}},{"name":"stderr","text":"167it [26:37,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b459095b96d4bacbb9af1589c50014c"}},"metadata":{}},{"name":"stderr","text":"168it [26:47,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291a5cf27c3840c58eb69b806ff501f1"}},"metadata":{}},{"name":"stderr","text":"169it [26:56,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed2efaae494469f8606db11741103ef"}},"metadata":{}},{"name":"stderr","text":"170it [27:06,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57d35d40a6c04801ae6f6f252d62e5fe"}},"metadata":{}},{"name":"stderr","text":"171it [27:15,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd7999ca04c4415798b76c054db3142a"}},"metadata":{}},{"name":"stderr","text":"172it [27:25,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d85796777d284db49a907e7670491446"}},"metadata":{}},{"name":"stderr","text":"173it [27:34,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3319b2f523746ac8bccba6ec7975946"}},"metadata":{}},{"name":"stderr","text":"174it [27:43,  9.44s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb7a366fb5d4a9ab6282975005a1659"}},"metadata":{}},{"name":"stderr","text":"175it [27:53,  9.48s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e5489d542df4172821b1b253ba85a90"}},"metadata":{}},{"name":"stderr","text":"176it [28:02,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23be1ad5c17545c3a62aa8bad21ac932"}},"metadata":{}},{"name":"stderr","text":"177it [28:12,  9.46s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4c27e40de94ddc9378cf46b4bd338b"}},"metadata":{}},{"name":"stderr","text":"178it [28:21,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf09689176814682a6c8226dbe2f81d5"}},"metadata":{}},{"name":"stderr","text":"179it [28:31,  9.49s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375a50914515494fbecec0367e4b130b"}},"metadata":{}},{"name":"stderr","text":"180it [28:40,  9.47s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec5b6536d5f48459c619d6d8d2c946f"}},"metadata":{}},{"name":"stderr","text":"181it [28:50,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffe09d5d0f14ae78eb83567f8cd8cef"}},"metadata":{}},{"name":"stderr","text":"182it [28:59,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f47dc70da3204bca8fa57a3c947c895e"}},"metadata":{}},{"name":"stderr","text":"183it [29:09,  9.50s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed79f6b8365480fbb62d8295963ce12"}},"metadata":{}},{"name":"stderr","text":"184it [29:19,  9.73s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4401211f6c42d28d7032fc34bda665"}},"metadata":{}},{"name":"stderr","text":"185it [29:29,  9.75s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac66f652e4ab44a68abd4546a2b96b21"}},"metadata":{}},{"name":"stderr","text":"186it [29:38,  9.66s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e7baf92b484f5c9fc281bfe134fe66"}},"metadata":{}},{"name":"stderr","text":"187it [29:48,  9.60s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99695a4d01364caaba0b71d4277dc68d"}},"metadata":{}},{"name":"stderr","text":"188it [29:57,  9.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380b18f6236e4c12b9cb57e8ce0ff189"}},"metadata":{}},{"name":"stderr","text":"189it [30:07,  9.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67599ea1eca4fe7bca0441c8daf2cdb"}},"metadata":{}},{"name":"stderr","text":"190it [30:16,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815f2f31609a432483d785644724aa74"}},"metadata":{}},{"name":"stderr","text":"191it [30:26,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"782795b1f6d54fb790d3896243bf074c"}},"metadata":{}},{"name":"stderr","text":"192it [30:35,  9.52s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef6589680924111b293603f4f727261"}},"metadata":{}},{"name":"stderr","text":"193it [30:45,  9.55s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b061381072cb45f5bf2b272f8b41d6f3"}},"metadata":{}},{"name":"stderr","text":"194it [30:55,  9.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2fba3f53d049ffb0cf7668f4eb9cf2"}},"metadata":{}},{"name":"stderr","text":"195it [31:04,  9.60s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e7bfa774dd4077b3d94661cf28f03a"}},"metadata":{}},{"name":"stderr","text":"196it [31:14,  9.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5778a32214ef4322bf8bb4ba47234159"}},"metadata":{}},{"name":"stderr","text":"197it [31:23,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959210f4c9714f6fb8180b5d011af745"}},"metadata":{}},{"name":"stderr","text":"198it [31:33,  9.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa8fc90b2fa149039601cff55c56ad0b"}},"metadata":{}},{"name":"stderr","text":"199it [31:42,  9.51s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"944e0d4a5124480687fbbf1dccd38358"}},"metadata":{}},{"name":"stderr","text":"200it [31:52,  9.56s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(couples[\"tweet_x\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Specifica il nome del file JSON\nfilename = '/kaggle/input/similarities/similarities_sbert.json'\n\ndef load_json(filename):\n    with open(filename, 'r') as file:\n        return json.load(file)\n\n# Carica i dati dal file JSON\ndata = load_json(filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creare un grafo vuoto\nG = nx.Graph()\n\nThreshold = 0.5 #threshold similarità\n\n# Itera su ogni record nel file JSON\nfor item in data:\n    dictionary = dict(item.items())\n    if not G.has_node(dictionary[\"user1\"]): #se utente non presente, lo aggiungo alla rete\n        G.add_node(dictionary[\"user1\"])\n    if not G.has_node(dictionary[\"user2\"]): #se utente non presente, lo aggiungo alla rete\n        G.add_node(dictionary[\"user2\"])\n    if float(dictionary[\"similarity\"])>Threshold:\n        G.add_edge(dictionary[\"user1\"], dictionary[\"user2\"], weight=float(dictionary[\"similarity\"]))\n    \n\nprint(f\"Number of nodes: {G.number_of_nodes()}\")\nprint(f\"Number of edges: {G.number_of_edges()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Disegnare il grafo\npos = nx.spring_layout(G)  # Posizionamento dei nodi\nweights = nx.get_edge_attributes(G, 'weight').values()\n\nnx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=5, font_size=5, font_weight='bold')\nnx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{d[\"weight\"]:.2f}' for u, v, d in G.edges(data=True)}, font_color='red')\nnx.draw_networkx_edges(G, pos, width=list(weights))\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Questa parte del codice prevedeva l'uso di Llama3, ma abbiamo visto non essere efficace in questo senso, quindi iul codice è stato commentato.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport subprocess\nimport threading\n\n!pip install langchain-community\n!pip install langchain-core\n\n#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh\n    \n#Avvio del server locale di Ollama\nt = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\nt.start()\n\n!ollama pull llama3\n\nt2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\nt2.start()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom langchain_community.llms import Ollama\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = \"You are a text-similarity evaluator. Your role is to analyze all the couple of tweets of users and calculate the semantic similarity between them. You must assign to each couple a decimal score from 0 (if the tweets are not similar) to 1 (if the tweets are similar). You have to give ONLY the number score, NOT anymore. If a tweet has offensive language, ignore it and DON'T answer. Give me a fast solution.\"\n\nllm = Ollama(\n    model=\"llama3\"\n)  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", prompt),\n    (\"user\", \"{input}\"),\n])\n\noutput_parser = StrOutputParser()\n\n\ndef ask_to_llama(tweet1,tweet2):   \n    #chain = template | llm | output_parser\n    \n    #response = chain.invoke({\"input\": \"Tweet 1:\" +tweet1+ \". Tweet 2:\" +tweet2})\n    response = llm.invoke(prompt + \"Tweet 1:\" +tweet1+ \". Tweet 2:\" +tweet2)\n    \n    return response\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport json\n\npartition = first_part #second_part #third_part #fourth_part #fifth_part\n# Specifica il nome del file JSON\nfilename = f'/kaggle/working/similarities_{partition}.json'\nrecords = []\n\n#aggiusta qui! non va bene df\ndf = pd.concat([partition.drop(columns=[\"user_followers_count\"]),last_1000.drop(columns=[\"user_followers_count\"])],axis=0)\nfor (user1, tweet1), (user2, tweet2) in tqdm(combinations(df.itertuples(index=False), 2)):\n    resp = ask_to_llama(tweet1,tweet2)\n    print(resp)\n    record = {\n        \"user1\": user1,\n        \"user2\": user2,\n        \"similarity\": resp\n    }\n    records.append(record)\n    \n# Scrivi i dati nel file JSON\nwith open(filename, 'w') as file:\n    json.dump(records, file, indent=4)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}