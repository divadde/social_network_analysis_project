{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4b3046",
   "metadata": {
    "papermill": {
     "duration": 0.02224,
     "end_time": "2024-07-14T14:07:14.256598",
     "exception": false,
     "start_time": "2024-07-14T14:07:14.234358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importazione librerie e visualizzazione Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b30ed9",
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-07-14T14:07:14.303237Z",
     "iopub.status.busy": "2024-07-14T14:07:14.302887Z",
     "iopub.status.idle": "2024-07-14T14:07:15.868994Z",
     "shell.execute_reply": "2024-07-14T14:07:15.868234Z"
    },
    "papermill": {
     "duration": 1.591795,
     "end_time": "2024-07-14T14:07:15.871433",
     "exception": false,
     "start_time": "2024-07-14T14:07:14.279638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af18f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:10.968451Z",
     "iopub.status.busy": "2024-07-14T12:49:10.968033Z",
     "iopub.status.idle": "2024-07-14T12:49:37.746500Z",
     "shell.execute_reply": "2024-07-14T12:49:37.745506Z",
     "shell.execute_reply.started": "2024-07-14T12:49:10.968426Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-07-14T14:07:15.893992",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lettura del dataset:\n",
    "df_trump = pd.read_csv(\"/kaggle/input/us-election-2020-tweets/hashtag_donaldtrump.csv\",lineterminator='\\n')\n",
    "df_biden = pd.read_csv(\"/kaggle/input/us-election-2020-tweets/hashtag_joebiden.csv\",lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2cf5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:37.748104Z",
     "iopub.status.busy": "2024-07-14T12:49:37.747757Z",
     "iopub.status.idle": "2024-07-14T12:49:37.753340Z",
     "shell.execute_reply": "2024-07-14T12:49:37.752448Z",
     "shell.execute_reply.started": "2024-07-14T12:49:37.748073Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Tweet with Trump hashtag: {len(df_trump)}\")\n",
    "print(f\"Tweet with Biden hashtag: {len(df_biden)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a74c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:37.756382Z",
     "iopub.status.busy": "2024-07-14T12:49:37.756053Z",
     "iopub.status.idle": "2024-07-14T12:49:39.339124Z",
     "shell.execute_reply": "2024-07-14T12:49:39.338066Z",
     "shell.execute_reply.started": "2024-07-14T12:49:37.756348Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dataframe unito (eliminati i duplicati)\n",
    "df_duplicated = pd.concat([df_trump,df_biden])\n",
    "df = df_duplicated.drop_duplicates(subset=\"tweet\")\n",
    "\n",
    "print(f\"Total tweets: {len(df_duplicated)}\")\n",
    "print(f\"Total tweets: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6c021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:39.340613Z",
     "iopub.status.busy": "2024-07-14T12:49:39.340295Z",
     "iopub.status.idle": "2024-07-14T12:49:39.374890Z",
     "shell.execute_reply": "2024-07-14T12:49:39.374050Z",
     "shell.execute_reply.started": "2024-07-14T12:49:39.340584Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1f0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:39.376507Z",
     "iopub.status.busy": "2024-07-14T12:49:39.376160Z",
     "iopub.status.idle": "2024-07-14T12:49:39.403460Z",
     "shell.execute_reply": "2024-07-14T12:49:39.402608Z",
     "shell.execute_reply.started": "2024-07-14T12:49:39.376473Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125844b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:39.404729Z",
     "iopub.status.busy": "2024-07-14T12:49:39.404451Z",
     "iopub.status.idle": "2024-07-14T12:49:39.525590Z",
     "shell.execute_reply": "2024-07-14T12:49:39.524690Z",
     "shell.execute_reply.started": "2024-07-14T12:49:39.404706Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Numero di utenti totali (potenziali nodi)\n",
    "print(df[\"user_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc429222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:39.527681Z",
     "iopub.status.busy": "2024-07-14T12:49:39.526891Z",
     "iopub.status.idle": "2024-07-14T12:49:49.576822Z",
     "shell.execute_reply": "2024-07-14T12:49:49.575835Z",
     "shell.execute_reply.started": "2024-07-14T12:49:39.527632Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_hashtags(tweet):\n",
    "    return re.findall(r'#\\w+', tweet.lower())\n",
    "\n",
    "df['hashtags'] = df['tweet'].apply(extract_hashtags)\n",
    "\n",
    "all_hashtags = [hashtag for hashtags in df['hashtags'] for hashtag in hashtags]\n",
    "\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "\n",
    "sorted_hashtag_counts = hashtag_counts.most_common()\n",
    "\n",
    "# Stampare la classifica degli hashtag\n",
    "print(\"Classifica degli hashtag più usati:\")\n",
    "for hashtag, count in sorted_hashtag_counts[:50]:\n",
    "    print(f\"{hashtag}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf4e22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Osservazioni:\n",
    "- Informazioni temporali che vanno dal 15 ottobre 2020 al 8 novembre 2020.\n",
    "- 481.000 potenziali nodi (filtraggio sulla base di like/retweet?)\n",
    "- Tweet scritti in diverse lingue (concentrarsi solo su quelli in inglese?)\n",
    "- Diversi valori mancanti nelle aree geografiche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e52b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing (filtraggio tweet/utenti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0651f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Probabilmente il primo filtraggio che occorre fare è quello sulla lingua. Potrebbe essere meglio considerare solo i tweet in inglese (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07fa87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:49.578185Z",
     "iopub.status.busy": "2024-07-14T12:49:49.577932Z",
     "iopub.status.idle": "2024-07-14T12:49:49.889320Z",
     "shell.execute_reply": "2024-07-14T12:49:49.888380Z",
     "shell.execute_reply.started": "2024-07-14T12:49:49.578162Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filtraggio sulla base dei like\n",
    "df_like_5 = df[df[\"likes\"]>=5]\n",
    "df_like_10 = df[df[\"likes\"]>=10]\n",
    "df_like_20 = df[df[\"likes\"]>=20]\n",
    "df_like_50 = df[df[\"likes\"]>=50]\n",
    "\n",
    "print(f\"Total tweets: {len(df_like_5)}\")\n",
    "print(f\"Total tweets: {len(df_like_10)}\")\n",
    "print(f\"Total tweets: {len(df_like_20)}\")\n",
    "print(f\"Total tweets: {len(df_like_50)}\")\n",
    "print(df_like_50[\"user_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe5431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:49.892127Z",
     "iopub.status.busy": "2024-07-14T12:49:49.891812Z",
     "iopub.status.idle": "2024-07-14T12:49:50.069691Z",
     "shell.execute_reply": "2024-07-14T12:49:50.068692Z",
     "shell.execute_reply.started": "2024-07-14T12:49:49.892102Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filtraggio sulla base dei retweet\n",
    "df_retweet_5 = df[df[\"retweet_count\"]>=5]\n",
    "df_retweet_10 = df[df[\"retweet_count\"]>=10]\n",
    "df_retweet_20 = df[df[\"retweet_count\"]>=20]\n",
    "df_retweet_50 = df[df[\"retweet_count\"]>=50]\n",
    "\n",
    "print(f\"Total tweets: {len(df_retweet_5)}\")\n",
    "print(f\"Total tweets: {len(df_retweet_10)}\")\n",
    "print(f\"Total tweets: {len(df_retweet_20)}\")\n",
    "print(f\"Total tweets: {len(df_retweet_50)}\")\n",
    "print(df_retweet_50[\"user_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81857145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:50.071008Z",
     "iopub.status.busy": "2024-07-14T12:49:50.070737Z",
     "iopub.status.idle": "2024-07-14T12:49:50.400442Z",
     "shell.execute_reply": "2024-07-14T12:49:50.399531Z",
     "shell.execute_reply.started": "2024-07-14T12:49:50.070984Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FILTRAGGIO BASATO SU paese=United states\n",
    "df_country= df[df[\"country\"]==\"United States of America\"]\n",
    "print(f\"Total tweets: {len(df_country)}\")\n",
    "\n",
    "print(df_country[\"user_id\"].value_counts())\n",
    "df_country.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd452b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:49:50.402032Z",
     "iopub.status.busy": "2024-07-14T12:49:50.401692Z",
     "iopub.status.idle": "2024-07-14T12:49:50.567451Z",
     "shell.execute_reply": "2024-07-14T12:49:50.566414Z",
     "shell.execute_reply.started": "2024-07-14T12:49:50.402001Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FILTRAGGIO BASATO SU stato!=null\n",
    "df_state= df_country[pd.notnull(df_country['state'])]\n",
    "print(f\"Total tweets: {len(df_country)}\")\n",
    "\n",
    "print(df_state[\"user_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fdfd8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Osservazione: sono pochi gli utenti che risiedono negli stati uniti e che hanno state==null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00647933",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Costruisco la rete con le menzioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ec06f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* Obiettivo: costruire una rete che tenga conto delle menzioni che provengono da utenti USA con più di 15.000 followers (potenzialmente i più influenti).\n",
    "* Obiettivo: costruire una rete che tenga conto delle menzioni che provengono da utenti USA con meno di 1.000 followers, studiamo comportamento tipico di persone meno famose.\n",
    "* Misurazione delle principali misure di centralità: in_degree, betweness, closeness.\n",
    "* Si potrebbe verificare con l'out_degree se sono presenti spam_farm (to do)\n",
    "* Degree distribution (to do)\n",
    "* Page rank (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b610683",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "popular = False #se true, considero rete con >15.000 followers, se false considero rete con <1.000 followers\n",
    "\n",
    "#FILTRAGGIO BASATO SU stato= United states e sul numero di follower, voglio capire se ci sono\n",
    "#utenti importanti o se ho completamente rimosso profili di informazione\n",
    "if popular:\n",
    "    df_country_e_follower= df_country[df_country[\"user_followers_count\"]>=15000]\n",
    "else:\n",
    "    df_country_e_follower= df_country[df_country[\"user_followers_count\"]<1000]\n",
    "print(f\"Total tweets: {len(df_country_e_follower)}\")\n",
    "print(df_country_e_follower[\"user_id\"].value_counts())\n",
    "df_country_e_follower.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ff071",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FILTRAGGIO BASATO SU MENZIONI+country+followers\n",
    "def contains_mentions(tweet):\n",
    "    return '@' in tweet\n",
    "\n",
    "df_with_mentions = df_country_e_follower[df_country_e_follower['tweet'].apply(contains_mentions)]\n",
    "\n",
    "print(f\"Total tweets: {len(df_with_mentions)}\")\n",
    "\n",
    "df_with_mentions.head()\n",
    "print(df_with_mentions[\"user_id\"].value_counts())\n",
    "#PS MI SONO ACCORTA CHE NON è BANALE REALIZZARE UN ARCO SE C'è UNA MENZIONE\n",
    "#DEVI RISALIRE AL USER ID DAL NOME \n",
    "#MA QUELL'UTENTE POTREBBE NON ESISTERE NEI DATI SE NON HA PUBBLICATO NIENTE (ci interessa davvero se abbia pubblicato qualcosa?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8d531",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Function to extract mentioned users from a tweet\n",
    "def extract_mentions(tweet):\n",
    "    return re.findall(r\"@(\\w+)\", tweet)\n",
    "\n",
    "# Add nodes and edges based on mentions\n",
    "for index, row in df_with_mentions.iterrows():\n",
    "    user_screen_name = row['user_screen_name'] #nome dell'utente\n",
    "    mentions = extract_mentions(row['tweet']) #menzioni dell'utente verso altri utenti\n",
    "    \n",
    "    # Add the user as a node\n",
    "    if not G.has_node(user_screen_name): #se utente non presente, lo aggiungo alla rete\n",
    "        G.add_node(user_screen_name)\n",
    "    \n",
    "    # Add edges from the user to each mentioned user if the mentioned user is already a node\n",
    "    for mention in mentions:\n",
    "        if not G.has_node(mention): #se il nodo menzionato non è presente, lo aggiungo alla rete\n",
    "            G.add_node(mention)\n",
    "        if mention!=user_screen_name: #rimuovo i selfloop (automenzioni)\n",
    "            G.add_edge(user_screen_name, mention)\n",
    "        \n",
    "\n",
    "# Display the number of nodes and edges\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b9f702",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "La rete è troppo grande, è il caso di applicare ulteriori filtraggi? (Per esempio, considerare utenti che hanno almeno un certo numero di menzioni), controllo con un parametro \"min_number_of_mentions\", elimino tutti i nodi che hanno un in_degree inferiore a una certa soglia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a750c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_number_of_mentions = 1\n",
    "\n",
    "in_degrees = dict(G.in_degree())\n",
    "nodes_to_remove = [node for node, degree in in_degrees.items() if degree < min_number_of_mentions]\n",
    "\n",
    "# Rimuovere i nodi dal grafo\n",
    "G.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75e316",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Non va bene questo approccio, elimino troppi archi all'interno della rete perdendo informazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230db9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the network\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.1)\n",
    "nx.draw(G, pos, with_labels=True, node_size=20, node_color='blue', font_size=10, font_color='white')\n",
    "plt.title(' Network utenti menzioni provenienti da tweet di utenti USA con >15000 follower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a37bdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Per ora la rete è orientata, quindi c'è un arco da n1 a n2 se n1 menziona n2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28394713",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Betweenness Centrality\n",
    "\"\"\"\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "betweenness_df = pd.DataFrame(list(betweenness_centrality.items()), columns=['user_screen_name', 'betweenness_centrality'])\n",
    "betweenness_df = betweenness_df.sort_values(by='betweenness_centrality', ascending=False)\n",
    "print(betweenness_df.head())\n",
    "\"\"\"\n",
    "\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Ordiniamo i nodi in base ai valori di betweenness centrality in ordine decrescente\n",
    "sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Stampiamo i nodi con i valori più alti di betweenness centrality\n",
    "for node, centrality in sorted_betweenness[:10]: #stampo solo i migliori 10\n",
    "    print(f'Nodo: {node}, Betweenness Centrality: {centrality:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9b1bd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Betweness centrality molto bassa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d55836",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Degree Centrality\n",
    "\"\"\"\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "degree_df = pd.DataFrame(list(degree_centrality.items()), columns=['user_screen_name', 'degree_centrality'])\n",
    "degree_df = degree_df.sort_values(by='degree_centrality', ascending=False)\n",
    "print(degree_df.head())\n",
    "\"\"\"\n",
    "\n",
    "degree_centrality = nx.in_degree_centrality(G)\n",
    "\n",
    "# Ordiniamo i nodi in base ai valori di degree centrality in ordine decrescente\n",
    "sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Stampiamo i nodi con i valori più alti di degree centrality\n",
    "for node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n",
    "    print(f'Nodo: {node}, Degree Centrality: {centrality:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdced36f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Closeness \n",
    "\"\"\"\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "closeness_df = pd.DataFrame(list(closeness_centrality.items()), columns=['user_screen_name', 'closeness_centrality'])\n",
    "closeness_df = closeness_df.sort_values(by='closeness_centrality', ascending=False)\n",
    "print(closeness_df.head())\n",
    "\"\"\"\n",
    "\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Ordiniamo i nodi in base ai valori di degree centrality in ordine decrescente\n",
    "sorted_degree = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Stampiamo i nodi con i valori più alti di degree centrality\n",
    "for node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n",
    "    print(f'Nodo: {node}, Closeness Centrality: {centrality:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0be70f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Rimuovendo i nodi isolati, la rete diventa più densa e le misure di centralità potrebbero aumentare per alcuni nodi. Questo accade perché la centralità è spesso una misura relativa e viene calcolata rispetto all'intera rete. Eliminare i nodi che non hanno connessioni (e quindi non contribuiscono alla rete) può far sì che i nodi rimanenti abbiano un impatto maggiore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51953858",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rimuovere i nodi isolati\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "# Display the number of nodes and edges\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "#Vengono rimossi pochi nodi (una ventina, probabilmente sono nodi che si automenzionano e basta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08321946",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Betweenness Centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "betweenness_df = pd.DataFrame(list(betweenness_centrality.items()), columns=['user_screen_name', 'betweenness_centrality'])\n",
    "betweenness_df = betweenness_df.sort_values(by='betweenness_centrality', ascending=False)\n",
    "print(betweenness_df.head())\n",
    "#Closeness \n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "closeness_df = pd.DataFrame(list(closeness_centrality.items()), columns=['user_screen_name', 'closeness_centrality'])\n",
    "closeness_df = closeness_df.sort_values(by='closeness_centrality', ascending=False)\n",
    "print(closeness_df.head())\n",
    "# Degree Centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "degree_df = pd.DataFrame(list(degree_centrality.items()), columns=['user_screen_name', 'degree_centrality'])\n",
    "degree_df = degree_df.sort_values(by='degree_centrality', ascending=False)\n",
    "print(degree_df.head())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38b332",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the network\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "#Nodi piu grandi sono associati a degree centrality maggiore\n",
    "node_size = [v * 10000 for v in degree_centrality.values()]\n",
    "\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_size=node_size, node_color='blue', font_size=10, font_color='black', edge_color='gray')\n",
    "plt.title(' Network utenti USA con >15000 follower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776789e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Costruisco la rete geografica\n",
    "* è una rete non orientata\n",
    "* Considero solo gli utenti negli USA e che hanno stato!=null\n",
    "* Inserisco un arco tra gli utenti dello stesso stato\n",
    "* Classifico ogni utente in pro-trump / pro-biden e lo coloro di rosso / blu\n",
    "* Creazione dei sottografi: Utilizziamo G.subgraph(nodes) per creare sottografi per ciascuno stato, selezionando i nodi che appartengono a quel particolare stato.\n",
    "* **Analisi degli stati con più sostenitori di Trump:** Conta il numero di nodi con preferenza politica \"Trump\" per ogni stato e stampa i risultati ordinati per numero decrescente.\n",
    "* **Fornire una predizione dell'esito delle elezioni e confrontarlo con ground trouth**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad3e0e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c6259",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Caricare il modello di sentiment analysis\n",
    "classifier = pipeline(\"text-classification\", model=\"DT12the/distilbert-sentiment-analysis\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c939c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definire una funzione per classificare i tweet\n",
    "#questa è approssimativa, perche se c'è un tweet con due tag?\n",
    "#inoltre devo considerare una lista di tag con tutte le varianti di tag \n",
    "def classify_tweet(tweet):\n",
    "    result = classifier(tweet)[0]\n",
    "    if 'Trump' in tweet:\n",
    "        return 'pro-Trump' if result['label'] == 'LABEL_0' else 'anti-Trump'\n",
    "    elif 'Biden' in tweet:\n",
    "        return 'pro-Biden' if result['label'] == 'LABEL_0' else 'anti-Biden'\n",
    "    else:\n",
    "        return 'neutral' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76cb6dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Costruzione rete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a518731",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.iloc[5]['tweet'])\n",
    "\n",
    "classify_tweet(df.iloc[5]['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ceb1d3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_country_e_follower= df_country[df_country[\"user_followers_count\"]<1000] #prendo gli utenti meno \"popolari\"\n",
    "print(f\"Total tweets: {len(df_country_e_follower)}\")\n",
    "print(df_country_e_follower[\"user_id\"].value_counts())\n",
    "\n",
    "#Concatenazione dei tweet per l'utente\n",
    "grouped_df = df_country_e_follower.groupby('user_id')['tweet'].apply(lambda tweets: ' '.join(tweets)).reset_index()\n",
    "print(f\"Total tweets after concate: {len(grouped_df)}\")\n",
    "\n",
    "#Drop colonna tweet dal primo dataframe\n",
    "df_dropped = df_country_e_follower.drop(columns=['tweet'])\n",
    "\n",
    "#Faccio la join per avere tutti i tweet insieme\n",
    "df_conc = pd.merge(df_dropped, grouped_df, on='user_id', how='inner')\n",
    "print(df_conc[\"user_id\"].value_counts())\n",
    "df_conc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b8c52",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "G_geo = nx.Graph() #NN orient\n",
    "\n",
    "# Aggiungi nodi (utenti degli USA)\n",
    "for index, row in df_conc.iterrows(): \n",
    "    if  pd.notnull(row['state']):\n",
    "        political_preference=classify_tweet(row['tweet'])\n",
    "        #print(political_preference)\n",
    "        G_geo.add_node(row['user_screen_name'], state=row['state'],\n",
    "                   political_preference=political_preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d36fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#aggiunta archi\n",
    "for u in G_geo.nodes():\n",
    "    for v in G_geo.nodes():\n",
    "        if u != v and G_geo.nodes[u]['state'] == G_geo.nodes[v]['state']:\n",
    "            G_geo.add_edge(u, v, relationship='same_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09c4d2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the number of nodes and edges\n",
    "print(f\"Number of nodes: {G_geo.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G_geo.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4c407",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Colorazione dei nodi in base alla political_preference\n",
    "node_colors = []\n",
    "for node in G_geo.nodes():\n",
    "    if G_geo.nodes[node]['political_preference'] == 'pro-Biden' or G_geo.nodes[node]['political_preference'] == 'anti-Trump':\n",
    "        node_colors.append('blue')  # Colore blu per i sostenitori di Biden\n",
    "    elif G_geo.nodes[node]['political_preference'] == 'pro-Trump'or G_geo.nodes[node]['political_preference'] == 'anti-Biden':\n",
    "        node_colors.append('red')   # Colore rosso per i sostenitori di Trump\n",
    "    else:\n",
    "        node_colors.append('gray')  # Colore grigio per i neutrali\n",
    "        \n",
    "# Disegna il grafo con i nodi colorati\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G_geo, k=0.1)\n",
    "nx.draw(G_geo, pos, with_labels=True, node_color=node_colors, node_size=20, font_size=0, font_color='black', edge_color='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e5d7b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Creazione dei sottografi per ciascuno stato\n",
    "state_graphs = {}\n",
    "for state in set(nx.get_node_attributes(G_geo, 'state').values()):\n",
    "    state_graphs[state] = G_geo.subgraph([n for n, d in G_geo.nodes(data=True) if d['state'] == state])\n",
    "\n",
    "# Disegna la rete con i cluster stati\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G_geo, k=0.1)\n",
    "\n",
    "# Genera una lista di colori\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(state_graphs)))\n",
    "\n",
    "# Disegna i sottografi per ciascuno stato\n",
    "for color, (state, subgraph) in zip(colors, state_graphs.items()):\n",
    "    nx.draw_networkx_nodes(subgraph, pos, node_size=20, label=state, node_color=[color] * subgraph.number_of_nodes())\n",
    "    nx.draw_networkx_edges(subgraph, pos, alpha=0.3)\n",
    "    \n",
    "\n",
    "plt.title('Rete sociale con cluster per stati')\n",
    "plt.legend(state_graphs.keys())\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29abc226",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Costruisco la rete di similarità con gli hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4e289",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Si pone il seguente problema: potrebbe non essere la scelta giusta andare a escludere utenti per numero di followers. Da un lato potremmo escludere il comportamento tipico degli utenti meno popolari, che sono anche quelli più numerosi (le persone comuni, che poi di fatto vanno a votare), dall'altro potremmo escludere il ruolo di utenti più popolari in grado di influenzare maggiormente gli altri utenti. Potremmo pensare di effettuare un campionamento casuale dei nodi per ridurre la dimensione della rete? Oppure dovremmo pensare al filtraggio sotto altri metodi (numero di like o retweet?). Potremmo fare anche un campionamento che si basa sulla degree distribution. Probabilmente la cosa migliore è andare a fare un campionamento casuale direttamente sul dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41672680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:51:58.683848Z",
     "iopub.status.busy": "2024-07-14T12:51:58.683458Z",
     "iopub.status.idle": "2024-07-14T12:52:04.051922Z",
     "shell.execute_reply": "2024-07-14T12:52:04.050980Z",
     "shell.execute_reply.started": "2024-07-14T12:51:58.683817Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_country[\"user_screen_name\"].value_counts())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "df_country_e_follower= df_country[df_country[\"user_followers_count\"]>10000] \n",
    "print(f\"Total tweets: {len(df_country_e_follower)}\")\n",
    "print(df_country_e_follower[\"user_id\"].value_counts())\n",
    "\"\"\"\n",
    "\n",
    "#Concatenazione dei tweet per l'utente\n",
    "grouped_df = df_country.groupby('user_screen_name')['tweet'].apply(lambda tweets: ' '.join(tweets)).reset_index()\n",
    "print(f\"Total tweets after concate: {len(grouped_df)}\")\n",
    "\n",
    "#Drop colonna tweet dal primo dataframe\n",
    "df_dropped = df_country.drop(columns=['tweet']) \n",
    "\n",
    "#Faccio la join per avere tutti i tweet insieme\n",
    "df_conc = pd.merge(df_dropped, grouped_df, on='user_screen_name', how='inner')\n",
    "print(len(df_conc))\n",
    "print(df_conc[\"user_screen_name\"].value_counts())\n",
    "df_conc.head()\n",
    "\n",
    "grouped_conc = df_country.groupby('user_screen_name')['tweet'].apply(lambda tweets: ' '.join(tweets)).reset_index()\n",
    "print(grouped_conc[\"user_screen_name\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d06a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:53:34.006195Z",
     "iopub.status.busy": "2024-07-14T12:53:34.005339Z",
     "iopub.status.idle": "2024-07-14T12:53:34.037277Z",
     "shell.execute_reply": "2024-07-14T12:53:34.036391Z",
     "shell.execute_reply.started": "2024-07-14T12:53:34.006155Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Effettuo un campionamento casuale del dataset (gli utenti sono troppi e non riusciremmo a costruire la rete)\n",
    "\n",
    "df_sampled = grouped_conc.sample(frac=0.2, random_state=42)\n",
    "print(df_sampled[\"user_screen_name\"].value_counts())\n",
    "\n",
    "df_sampled.head()\n",
    "# Idea di altro campionamento: \n",
    "# stimo i degree in modo parallelo (calcolo similarità dei primi 100 utenti con tutti gli altri)\n",
    "# campiono seguendo la stima della distribuzione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1ad31",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Osservazione: bisognerebbe forse creare dei macro-hashtag. Hashtag simili dovrebbero appartenere a un unico hashtag più generale. Per ora costruiamo la rete senza tener conto di questo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862650b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:05:01.552366Z",
     "iopub.status.busy": "2024-07-14T13:05:01.551512Z",
     "iopub.status.idle": "2024-07-14T13:05:01.847101Z",
     "shell.execute_reply": "2024-07-14T13:05:01.846107Z",
     "shell.execute_reply.started": "2024-07-14T13:05:01.552318Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funzione per estrarre gli hashtag da un tweet\n",
    "def extract_hashtags(tweet):\n",
    "    return re.findall(r'#\\w+', tweet.lower())\n",
    "\n",
    "# Aggiungere una colonna con gli hashtag estratti\n",
    "user_hashtags = df_sampled.copy()\n",
    "user_hashtags['hashtags'] = df_sampled['tweet'].apply(extract_hashtags)\n",
    "\n",
    "# Trasformo la lista di tweet in un insieme (per non avere duplicati)\n",
    "user_hashtags[\"hashtags\"] = user_hashtags['hashtags'].apply(set)\n",
    "\n",
    "print(len(user_hashtags))\n",
    "print(user_hashtags[\"user_screen_name\"].value_counts())\n",
    "user_hashtags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42af1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:05:28.998988Z",
     "iopub.status.busy": "2024-07-14T13:05:28.998614Z",
     "iopub.status.idle": "2024-07-14T13:09:46.131780Z",
     "shell.execute_reply": "2024-07-14T13:09:46.130746Z",
     "shell.execute_reply.started": "2024-07-14T13:05:28.998959Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Threshold = 0.3\n",
    "\n",
    "# Funzione per calcolare la similarità di Jaccard\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union\n",
    "\n",
    "df_final = user_hashtags.drop(columns=[\"tweet\"])\n",
    "# Calcolare la similarità di Jaccard tra ogni coppia di utenti\n",
    "edges = []\n",
    "for (user1, hashtags1), (user2, hashtags2) in combinations(user_hashtags.drop(columns=[\"tweet\"]).itertuples(index=False), 2):\n",
    "    similarity = jaccard_similarity(hashtags1, hashtags2)\n",
    "    if similarity > Threshold:  # Aggiungere solo archi con similarità positiva\n",
    "        edges.append((user1, user2, similarity))\n",
    "\n",
    "# Creare un grafo vuoto\n",
    "G = nx.Graph()\n",
    "\n",
    "# Aggiungere nodi (utenti)\n",
    "for user in df_final['user_screen_name']: \n",
    "    G.add_node(user)\n",
    "\n",
    "# Aggiungere archi con pesi (similarità di Jaccard)\n",
    "for user1, user2, weight in edges:\n",
    "    G.add_edge(user1, user2, weight=weight)\n",
    "    \n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a85536",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disegnare il grafo (non si capisce niente, troppi nodi dentro la rete)\n",
    "pos = nx.spring_layout(G)  # Posizionamento dei nodi\n",
    "weights = nx.get_edge_attributes(G, 'weight').values()\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=5, font_size=5, font_weight='bold')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{d[\"weight\"]:.2f}' for u, v, d in G.edges(data=True)}, font_color='red')\n",
    "nx.draw_networkx_edges(G, pos, width=list(weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac530d50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot della degree distribution\n",
    "\n",
    "# Calcolare i gradi dei nodi\n",
    "degrees = [degree for node, degree in G.degree()]\n",
    "\n",
    "# Calcolare la distribuzione dei gradi\n",
    "degree_count = Counter(degrees)\n",
    "deg, cnt = zip(*degree_count.items())\n",
    "\n",
    "# Fare il plot della distribuzione dei gradi\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(deg, cnt, width=10, color='b')\n",
    "\n",
    "plt.title(\"Degree Distribution\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3cdf5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Osserviamo la presenza di una power law, ma probabilmente ci sono 3 componenti giganti connesse! Provo a estrarre utenti che fanno parte di quelle componenti e vedo i loro hashtags per confermare la presenza di componenti giganti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcd181",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(degree_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba27b0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prendiamo gli utenti che hanno degree 998.\n",
    "\n",
    "desired_degree = 767 #950, #767, #998\n",
    "\n",
    "# Filtrare i nodi che hanno il grado specificato\n",
    "nodes_with_desired_degree = [node for node, degree in degree_dict.items() if degree == desired_degree]\n",
    "\n",
    "df_giant = df_final[df_final[\"user_screen_name\"].isin(nodes_with_desired_degree)]\n",
    "# Stampare i nodi con il grado desiderato\n",
    "print(f\"Nodi con grado {df_giant}:\")\n",
    "print(df_giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ca77c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Come sospettato, le componenti connesse sono legate agli hashtag #trump, #biden, #joebiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fec35a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot della weighted degree\n",
    "\n",
    "# Calcolare il weighted degree dei nodi\n",
    "weighted_degrees = dict(G.degree(weight='weight'))\n",
    "\n",
    "# Calcolare la distribuzione del weighted degree\n",
    "weighted_degree_count = Counter(weighted_degrees.values())\n",
    "deg, cnt = zip(*weighted_degree_count.items())\n",
    "\n",
    "# Fare il plot della distribuzione del weighted degree\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(deg, cnt, width=10, color='b')\n",
    "\n",
    "plt.title(\"Weighted Degree Distribution\")\n",
    "plt.xlabel(\"Weighted Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edc85e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Si potrebbe effettuare un campionamento dei nodi tenendo conto della degree distribution dei nodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbbfdf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definire la funzione di campionamento basato sui gradi\n",
    "\"\"\"\n",
    "def degree_based_sampling(graph, sample_size):\n",
    "    # Calcolare i gradi dei nodi\n",
    "    degrees = dict(graph.degree())\n",
    "    nodes, degree_values = zip(*degrees.items())\n",
    "    \n",
    "    # Convertire i gradi in probabilità (più alto il grado, maggiore la probabilità di essere selezionato)\n",
    "    total_degree = sum(degree_values)\n",
    "    probabilities = [degree / total_degree for degree in degree_values]\n",
    "    \n",
    "    # Campionare i nodi in base alle probabilità\n",
    "    sampled_nodes = np.random.choice(nodes, size=sample_size, replace=False, p=probabilities)\n",
    "    \n",
    "    # Restituire il sottografo campionato\n",
    "    return graph.subgraph(sampled_nodes)\n",
    "\n",
    "# Campionare il 20% dei nodi basato sui gradi\n",
    "sample_size = int(len(G.nodes) * 0.2)\n",
    "G_sampled = degree_based_sampling(G, sample_size)\n",
    "\n",
    "# Calcolare la distribuzione dei gradi nel grafo campionato\n",
    "sampled_degrees = [degree for node, degree in G_sampled.degree()]\n",
    "sampled_degree_count = Counter(sampled_degrees)\n",
    "sampled_deg, sampled_cnt = zip(*sampled_degree_count.items())\n",
    "\n",
    "# Fare il plot della distribuzione dei gradi nel grafo campionato\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(sampled_deg, sampled_cnt, width=0.80, color='b')\n",
    "\n",
    "plt.title(\"Degree Distribution in Sampled Graph\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Fare il plot della distribuzione dei gradi nel grafo originale per confronto\n",
    "original_degrees = [degree for node, degree in G.degree()]\n",
    "original_degree_count = Counter(original_degrees)\n",
    "orig_deg, orig_cnt = zip(*original_degree_count.items())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(orig_deg, orig_cnt, width=0.80, color='r')\n",
    "\n",
    "plt.title(\"Degree Distribution in Original Graph\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37d8b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Analisi con le misure di centralità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a51bb6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Degree Centrality\n",
    "\"\"\"\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "degree_df = pd.DataFrame(list(degree_centrality.items()), columns=['user_screen_name', 'degree_centrality'])\n",
    "degree_df = degree_df.sort_values(by='degree_centrality', ascending=False)\n",
    "print(degree_df.head())\n",
    "\"\"\"\n",
    "\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# Ordiniamo i nodi in base ai valori di degree centrality in ordine decrescente\n",
    "sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Stampiamo i nodi con i valori più alti di degree centrality\n",
    "for node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n",
    "    print(f'Nodo: {node}, Degree Centrality: {centrality:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713450d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Closeness \n",
    "\"\"\"\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "closeness_df = pd.DataFrame(list(closeness_centrality.items()), columns=['user_screen_name', 'closeness_centrality'])\n",
    "closeness_df = closeness_df.sort_values(by='closeness_centrality', ascending=False)\n",
    "print(closeness_df.head())\n",
    "\"\"\"\n",
    "\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Ordiniamo i nodi in base ai valori di degree centrality in ordine decrescente\n",
    "sorted_degree = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Stampiamo i nodi con i valori più alti di degree centrality\n",
    "for node, centrality in sorted_degree[:10]: #stampo solo i migliori 10\n",
    "    print(f'Nodo: {node}, Closeness Centrality: {centrality:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe677f64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Community detection con la rete tra gli hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d1dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:09:46.134323Z",
     "iopub.status.busy": "2024-07-14T13:09:46.133938Z",
     "iopub.status.idle": "2024-07-14T13:13:08.826580Z",
     "shell.execute_reply": "2024-07-14T13:13:08.825633Z",
     "shell.execute_reply.started": "2024-07-14T13:09:46.134288Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import community as community_louvain\n",
    "\n",
    "# Eseguire la community detection usando l'algoritmo di Louvain\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "\"\"\"\n",
    "# Disegnare il grafo con le comunità\n",
    "pos = nx.spring_layout(G)\n",
    "cmap = plt.get_cmap('viridis')\n",
    "nx.draw_networkx_nodes(G, pos, node_size=5, cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c2af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:13:08.828268Z",
     "iopub.status.busy": "2024-07-14T13:13:08.827929Z",
     "iopub.status.idle": "2024-07-14T13:13:08.842998Z",
     "shell.execute_reply": "2024-07-14T13:13:08.841982Z",
     "shell.execute_reply.started": "2024-07-14T13:13:08.828243Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "communities = {}\n",
    "for node, community in partition.items():\n",
    "    if community not in communities:\n",
    "        communities[community] = []\n",
    "    communities[community].append(node)\n",
    "\n",
    "\"\"\"\n",
    "for community, nodes in communities.items():\n",
    "    print(f\"Community {community}:\")\n",
    "    print(\", \".join(nodes))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fa929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:13:08.846149Z",
     "iopub.status.busy": "2024-07-14T13:13:08.845402Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vedo le community che hanno almeno 10 nodi:\n",
    "\n",
    "# Filtrare le comunità che hanno almeno 10 nodi\n",
    "large_communities = {community: nodes for community, nodes in communities.items() if len(nodes) >= 10}\n",
    "\n",
    "# Stampare il nome dei nodi di ogni comunità con almeno 10 nodi\n",
    "for community, nodes in large_communities.items():\n",
    "    print(f\"Community {community} (size: {len(nodes)}):\")\n",
    "    print(\", \".join(nodes))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eafa3d",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-07-14T13:13:08.932346Z",
     "shell.execute_reply": "2024-07-14T13:13:08.931458Z",
     "shell.execute_reply.started": "2024-07-14T13:13:08.866621Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Studiamo gli hashtags più frequenti per ogni community più numerosa\n",
    "\n",
    "for community, nodes in large_communities.items():\n",
    "    print(f\"Community: {community}\")\n",
    "    print(f\"Num nodes: {len(nodes)}\")\n",
    "\n",
    "    df_comm = user_hashtags[user_hashtags[\"user_screen_name\"].isin(nodes)]\n",
    "    print(df_comm[\"user_screen_name\"].value_counts())\n",
    "\n",
    "    all_hashtags = [hashtag for hashtags in df_comm['hashtags'] for hashtag in hashtags]\n",
    "\n",
    "    hashtag_counts = Counter(all_hashtags)\n",
    "\n",
    "    sorted_hashtag_counts = hashtag_counts.most_common()\n",
    "\n",
    "    # Stampare la classifica degli hashtag\n",
    "    print(\"Classifica degli hashtag più usati:\")\n",
    "    for hashtag, count in sorted_hashtag_counts[:50]:\n",
    "        print(f\"{hashtag}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f030e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:26:07.152608Z",
     "iopub.status.busy": "2024-07-14T13:26:07.152245Z",
     "iopub.status.idle": "2024-07-14T13:27:40.335607Z",
     "shell.execute_reply": "2024-07-14T13:27:40.334710Z",
     "shell.execute_reply.started": "2024-07-14T13:26:07.152579Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Rimozione di URL, menzioni e hashtag\n",
    "    text = re.sub(r\"http\\S+|@\\S+|#\\S+\", \"\", text)\n",
    "    # Rimozione di punteggiatura e numeri\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    # Convertire il testo in minuscolo\n",
    "    text = text.lower()\n",
    "    # Tokenizzazione\n",
    "    tokens = word_tokenize(text)\n",
    "    # Rimozione delle stopword\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Analizzare le comunità per determinare i topic\n",
    "for community, nodes in large_communities.items():\n",
    "    all_words = []\n",
    "    for node in nodes:\n",
    "        all_words.extend(preprocess_text(user_hashtags.loc[user_hashtags[\"user_screen_name\"]==node, 'tweet'].values[0]))\n",
    "    word_counts = Counter(all_words)\n",
    "    most_common_words = word_counts.most_common(30)\n",
    "    print(f\"Community {community} (size: {len(nodes)}):\")\n",
    "    print(\"Most common words:\", most_common_words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd22f40",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Creazione macro-hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab3f64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Estraggo tutti gli hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbca1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T09:22:17.800393Z",
     "iopub.status.busy": "2024-07-14T09:22:17.800003Z",
     "iopub.status.idle": "2024-07-14T09:22:17.854074Z",
     "shell.execute_reply": "2024-07-14T09:22:17.853077Z",
     "shell.execute_reply.started": "2024-07-14T09:22:17.800363Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_hashtags = [hashtag for hashtags in user_hashtags['hashtags'] for hashtag in hashtags]\n",
    "\n",
    "hashtag_counts = Counter(all_hashtags)\n",
    "\n",
    "#rimuovo tutti gli hashtags che sono stati usati meno di 5 volte.\n",
    "filtered_hashtags = [hashtag for hashtag, count in hashtag_counts.items() if count >= 30]\n",
    "\"\"\"\n",
    "sorted_hashtag_counts = filtered_hashtag_counts.most_common()\n",
    "\n",
    "# Stampare la classifica degli hashtag\n",
    "print(\"Classifica degli hashtag più usati:\")\n",
    "for hashtag, count in sorted_hashtag_counts[:50]:\n",
    "    print(f\"{hashtag}: {count}\")\n",
    "\"\"\"\n",
    "print(f\"Number of hashtags:{len(hashtag_counts)}\")\n",
    "print(f\"Number of hashtags:{len(filtered_hashtags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7929c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Usiamo LLAMA3 per individuare quelli che possono essere dei macrohashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adad02",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Problema, l'output viene sempre troncato perché è troppo lungo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a0f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T09:22:21.711454Z",
     "iopub.status.busy": "2024-07-14T09:22:21.711073Z",
     "iopub.status.idle": "2024-07-14T09:22:52.665726Z",
     "shell.execute_reply": "2024-07-14T09:22:52.664478Z",
     "shell.execute_reply.started": "2024-07-14T09:22:21.711424Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "\n",
    "!pip install langchain-community\n",
    "!pip install langchain-core\n",
    "\n",
    "#istallazione di ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "    \n",
    "#Avvio del server locale di Ollama\n",
    "t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n",
    "t.start()\n",
    "\n",
    "!ollama pull llama3\n",
    "\n",
    "t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf93d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T09:22:52.669594Z",
     "iopub.status.busy": "2024-07-14T09:22:52.669172Z",
     "iopub.status.idle": "2024-07-14T09:22:52.679991Z",
     "shell.execute_reply": "2024-07-14T09:22:52.679003Z",
     "shell.execute_reply.started": "2024-07-14T09:22:52.669552Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = \"You are an hashtag evaluator. Your role is to analyze all the hashtags and group them in macro-categories of hashtags, which are the most general.\"\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "def hashtags_to_llama(hashtags):   \n",
    "    #chain = template | llm | output_parser\n",
    "    \n",
    "    #response = chain.invoke({\"input\": \"Tweet 1:\" +tweet1+ \". Tweet 2:\" +tweet2})\n",
    "    response = llm.invoke(prompt + \"Hashtags:\" + hashtags)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099e16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T09:22:52.681703Z",
     "iopub.status.busy": "2024-07-14T09:22:52.681425Z",
     "iopub.status.idle": "2024-07-14T09:23:11.366003Z",
     "shell.execute_reply": "2024-07-14T09:23:11.365026Z",
     "shell.execute_reply.started": "2024-07-14T09:22:52.681679Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specifica il nome del file JSON\n",
    "filename = '/kaggle/working/hashtags.json'\n",
    "hashtags_list = \"\"\n",
    "\n",
    "\n",
    "for hashtag in hashtag_counts:\n",
    "    hashtags_list = hashtags_list + \" \" + hashtag \n",
    "#print(hashtags_list)\n",
    "    \n",
    "resp = hashtags_to_llama(hashtags_list)\n",
    "print(resp)\n",
    "    \n",
    "# Scrivi i dati nel file JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(resp, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0d561",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e2311",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Vogliamo scoprire i topic principali usando una community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb79d71",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db85ae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Costruisco la rete con similarità usando language model (llama3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c566e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_df = df_country.groupby('user_screen_name')['tweet'].apply(lambda tweets: ' '.join(tweets)).reset_index()\n",
    "print(f\"Total tweets after concate: {len(grouped_df)}\")\n",
    "print(grouped_df[\"user_screen_name\"].value_counts())\n",
    "\n",
    "df_sampled = grouped_df.sample(frac=0.2, random_state=42) #\n",
    "print(df_sampled[\"user_screen_name\"].value_counts())\n",
    "\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ceec75",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Utilizzo pipeline transformers per filtrare tutti i tweet che non sono in inglese. (Non funziona correttamente!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a670cdc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "\n",
    "filename=\"/kaggle/working/user_to_filter.json\"\n",
    "user_to_filter = []\n",
    "\n",
    "for row in tqdm(df_sampled.itertuples(index=True, name='Pandas')):\n",
    "    candidate_labels = ['english language', 'not english language']\n",
    "    resp = classifier(row.tweet, candidate_labels)[\"labels\"][0]\n",
    "    print(row.tweet)\n",
    "    print(resp)\n",
    "    if resp == \"not english language\":\n",
    "        record = {\n",
    "            \"user\": row.user_screen_name\n",
    "        }\n",
    "        user_to_filter.append(record)\n",
    "\n",
    "    \n",
    "# Scrivi i dati nel file JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(user_to_filter, file)\n",
    "\n",
    "#crea nuovo df leggendo json con utenti da eliminare\n",
    "\n",
    "# Funzione per caricare il contenuto di un file JSON\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Specifica il nome del file JSON\n",
    "filename=\"/kaggle/working/user_to_filter.json\"\n",
    "\n",
    "# Carica i dati dal file JSON\n",
    "data = load_json(filename)\n",
    "user_to_filter = []\n",
    "\n",
    "# Itera su ogni record nel file JSON\n",
    "for item in data:\n",
    "    dictionary = dict(item.items())\n",
    "    user_to_filter.append(dictionary[\"user\"])\n",
    "\n",
    "# Elimino da grouped_df gli utenti che non hanno tweet in inglese\n",
    "indexes = grouped_df[gouped_df['user_screen_name'].isin(user_to_filter)].index\n",
    "\n",
    "# Eliminare le righe usando il metodo drop\n",
    "df_filtered = grouped_df.drop(indexes)\n",
    "\n",
    "print(f\"Users before filter: {len(grouped_df)}\")\n",
    "print(f\"Users after filter: {len(df_filtered)}\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7e89b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Utilizzo pipeline per la summarization per testi troppo lunghi. Problematica, alcuni testi sono eccessivamente lunghi e il modello va out of memory. Occorre trovare una strategia alternativa. Al posto di usare un modello, potremmo semplicemente scegliere di non concatenare oltre un certo numero di tweet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92400fca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "summarizer = pipeline(task=\"summarization\", model=\"google-t5/t5-base\", tokenizer=\"google-t5/t5-base\", device=device)\n",
    "\n",
    "#3000 non va bene, probabilmente occorre abbassarla ulteriormente\n",
    "Threshold = 3000 #soglia sul numero di caratteri, se viene superata questa soglia, il testo viene riassunto\n",
    "\n",
    "filename=\"/kaggle/working/summarization.json\"\n",
    "summarized = []\n",
    "\n",
    "for row in tqdm(df_sampled.itertuples(index=True, name='Pandas')): #df_filtered\n",
    "    if (len(row.tweet)>Threshold):\n",
    "        text = row.tweet\n",
    "        if (len(row.tweet)>10000): #se il testo è oltre i 10.000 caratteri, lo tronco\n",
    "            text = text[:10000]\n",
    "        #print(text)\n",
    "        resp = summarizer(text)\n",
    "        #print(resp)\n",
    "        record = {\n",
    "            \"user\": row.user_screen_name,\n",
    "            \"summerized\": resp[0][\"summary_text\"]\n",
    "        }\n",
    "        summarized.append(record)\n",
    "\n",
    "    \n",
    "# Scrivi i dati nel file JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(summarized, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6532b5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(summarized, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3ed5e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Codice per sostituire \n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Specifica il nome del file JSON\n",
    "filename=\"/kaggle/working/summarization.json\"\n",
    "\n",
    "# Carica i dati dal file JSON\n",
    "data = load_json(filename)\n",
    "\n",
    "# Itera su ogni record nel file JSON\n",
    "for item in data:\n",
    "    dictionary = dict(item.items())\n",
    "    df_sampled.loc[df_sampled['user_screen_name'] == dictionary[\"user\"], 'tweet'] = dictionary[\"summerized\"]\n",
    "    \n",
    "#controllo per vedere se sono rimasti tweet con più di 3000 caratteri    \n",
    "for row in df_sampled.itertuples(index=True, name='Pandas'): #df_filtered\n",
    "    if (len(row.tweet)>Threshold):\n",
    "        print(\"Tweet con più di 3000 caratteri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff3938",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(df_country[\"user_screen_name\"].value_counts())\n",
    "\n",
    "prompt = \"You are a translator. Your role is to analyze all the tweets of users and write the name of the user if he doesn't speak english. You must write ONLY the name of the user if he doesn't speak english and not anymore.\"\n",
    "\n",
    "def llama_filter(user,tweet,prompt):   \n",
    "    \n",
    "    full_prompt = prompt + \"User name: \" + user + \". Tweet:\" + tweet\n",
    "    \n",
    "    response = requests.post('http://localhost:11434/api/generate', \n",
    "                             data=json.dumps({'model': 'llama3', 'prompt': full_prompt, 'stream': False}), \n",
    "                             headers={'Content-Type': 'application/json'})\n",
    "    \n",
    "    return response.json()['response']\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Iniziare il cronometro\n",
    "start_time = time.time()\n",
    "\n",
    "# Specifica il nome del file JSON\n",
    "filename = '/kaggle/working/filter_users.json'\n",
    "records = []\n",
    "\n",
    "for row in df_country.itertuples(index=True, name='Pandas'):\n",
    "    resp = llama_filter(row.user_screen_name,row.tweet,prompt)\n",
    "    record = {\n",
    "        \"user\": resp\n",
    "    }\n",
    "    records.append(record)\n",
    "    \n",
    "# Scrivi i dati nel file JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(records, file)\n",
    "\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "# Calcolare il tempo di esecuzione\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tempo di esecuzione: {execution_time} secondi\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b81e7a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Prova con Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e26e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "\n",
    "!pip install langchain-community\n",
    "!pip install langchain-core\n",
    "\n",
    "#istallazione di ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "    \n",
    "#Avvio del server locale di Ollama\n",
    "t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n",
    "t.start()\n",
    "\n",
    "!ollama pull llama3\n",
    "\n",
    "t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac3aae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = \"You are a text-similarity evaluator. Your role is to analyze all the couple of tweets of users and calculate the semantic similarity between them. You must assign to each couple a decimal score from 0 (if the tweets are not similar) to 1 (if the tweets are similar). You have to give ONLY the number score, not anymore. If a tweet has offensive language, If a tweet has offensive language, ignore it and DON'T answer. Give me a fast solution.\"\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "def ask_to_llama(tweet1,tweet2):   \n",
    "    #chain = template | llm | output_parser\n",
    "    \n",
    "    #response = chain.invoke({\"input\": \"Tweet 1:\" +tweet1+ \". Tweet 2:\" +tweet2})\n",
    "    response = llm.invoke(prompt + \"Tweet 1:\" +tweet1+ \". Tweet 2:\" +tweet2)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb66754",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specifica il nome del file JSON\n",
    "filename = '/kaggle/working/similarities.json'\n",
    "records = []\n",
    "\n",
    "for (user1, tweet1), (user2, tweet2) in tqdm(combinations(df_sampled.itertuples(index=False), 2)):\n",
    "    resp = ask_to_llama(tweet1,tweet2)\n",
    "    print(resp)\n",
    "    record = {\n",
    "        \"user1\": user1,\n",
    "        \"user2\": user2,\n",
    "        \"similarity\": resp\n",
    "    }\n",
    "    records.append(record)\n",
    "    \n",
    "# Scrivi i dati nel file JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(records, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5eade3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Prova senza Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508477d8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "#istallazione di ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "#un thread demone avvia il server locale di ollama\n",
    "t = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\n",
    "t.start()\n",
    "\n",
    "#un altro thread demone avvia llama3\n",
    "!ollama pull llama3\n",
    "t2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98ba7d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask_to_llama(tweet1,tweet2,prompt):   \n",
    "    \n",
    "    full_prompt = prompt + \"Tweet 1: \" + tweet1 + \". Tweet 2:\" + tweet2\n",
    "    \n",
    "    response = requests.post('http://localhost:11434/api/generate', \n",
    "                             data=json.dumps({'model': 'llama3', 'prompt': full_prompt, 'stream': False}), \n",
    "                             headers={'Content-Type': 'application/json'})\n",
    "    \n",
    "    return response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4a0ec6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prova\n",
    "\n",
    "\"\"\"\n",
    "resp = ask_to_llama(\"nicola is stupid, and he is really bold\", \"smart person and with a lot of hair\", prompt)\n",
    "print(resp)\n",
    "\n",
    "filename = '/kaggle/working/similarities.json'\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(resp, file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb552ec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "prompt = \"You are a text-similarity evaluator. Your role is to analyze all the couple of tweets of users and calculate the semantic similarity between them. You must assign to each couple a decimal score from 0 (if the tweets are not similar) to 1 (if the tweets are similar). You have to give ONLY the number score, not anymore. Let's think step by step, and taking the just amount of time you need to evaluate at the best of your capabilities.\"\n",
    "prompt2 = \"You are a text-similarity evaluator. Your role is to analyze all the couple of tweets of users and calculate the semantic similarity between them. You must assign to each couple a decimal score from 0 (if the tweets are not similar) to 1 (if the tweets are similar). You have to give ONLY the number score, not anymore. Give me a fast solution.\"\n",
    "\n",
    "# Iniziare il cronometro\n",
    "start_time = time.time()\n",
    "\n",
    "# Specifica il nome del file JSON\n",
    "filename = '/kaggle/working/similarities.json'\n",
    "records = []\n",
    "\n",
    "for (user1, tweet1), (user2, tweet2) in tqdm(combinations(df_sampled.itertuples(index=False), 2)):\n",
    "    resp = ask_to_llama(tweet1,tweet2,prompt2)\n",
    "    print(resp)\n",
    "    record = {\n",
    "        \"user1\": user1,\n",
    "        \"user2\": user2,\n",
    "        \"similarity\": resp\n",
    "    }\n",
    "    records.append(record)\n",
    "    \n",
    "# Scrivi i dati nel file JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(records, file)\n",
    "\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "# Calcolare il tempo di esecuzione\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tempo di esecuzione: {execution_time} secondi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d91df4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "PROBLEMA: SI IMPIEGA TROPPO TEMPO, SERVONO MENO NODI! calcola un 10/15 secondi per coppia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7081a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Vediamo per la classificazione delle preferenze quanto impiega. (IDEA: prova a dare più tweet insieme!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777825d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sampled.iloc[21][\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6cd16f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def preference_llama(tweet,prompt):   \n",
    "    \n",
    "    full_prompt = prompt + \"TWEET LISTS: \" + tweet\n",
    "    \n",
    "    response = requests.post('http://localhost:11434/api/generate', \n",
    "                             data=json.dumps({'model': 'llama3', 'prompt': full_prompt, 'stream': False}), \n",
    "                             headers={'Content-Type': 'application/json'})\n",
    "    \n",
    "    return response.json()['response']\n",
    "\n",
    "prompt = \"You are a political classifier over a list of tweets about USA election. Your role is to analyze the list of tweets of users and to establish if user is Pro-Biden or Pro-Trump. Each tweet start when you read: 'TWEET START'. You must assign TO EACH tweet a class (Pro-Biden or Pro-Trump). You have to give ONLY the class for EACH tweet, NOT ANYMORE. The class for each tweet must be separated by a comma. If a tweet has offensive language, ignore it and predict the class for this tweet as 'X'.\"\n",
    "\n",
    "# Specifica il nome del file JSON\n",
    "filename = '/kaggle/working/preferences.json'\n",
    "records = []\n",
    "\"\"\"\n",
    "for index, row in df_sampled.iterrows(): \n",
    "    resp = preference_llama(row.tweet,prompt)\n",
    "    print(resp)\n",
    "    record = {\n",
    "        \"user\": row.user_screen_name,\n",
    "        \"class\": resp\n",
    "    }\n",
    "    records.append(record)\n",
    "\"\"\"\n",
    "\n",
    "#prova con lista di tweet\n",
    "tweet_list=[]\n",
    "counter=0\n",
    "max_list=4\n",
    "for index, row in df_sampled.iterrows(): \n",
    "    #print(row)\n",
    "    tweet_list.append(row)\n",
    "    if counter<max_list-1:\n",
    "        counter=counter+1\n",
    "    else:\n",
    "        counter2=0\n",
    "        tweets=\"TWEET START: \"\n",
    "        for row in tweet_list:\n",
    "            counter2=counter2+1\n",
    "            if(counter2==counter):\n",
    "                tweets=tweets+row.tweet+\".\"\n",
    "            else:\n",
    "                tweets=tweets+row.tweet+\". TWEET START:\"\n",
    "        resp = preference_llama(tweets,prompt)\n",
    "        print(resp)\n",
    "        for row in tweet_list:\n",
    "            record = {\n",
    "                \"user\": row.user_screen_name,\n",
    "                \"class\": resp\n",
    "            }\n",
    "            records.append(record)\n",
    "        counter=0\n",
    "        tweet_list.clear()\n",
    "\n",
    "    \n",
    "# Scrivi i dati nel file JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(records, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77250f09",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specifica il nome del file JSON\n",
    "filename = '/kaggle/working/similarities.json'\n",
    "\n",
    "# Carica i dati dal file JSON\n",
    "data = load_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f506f42",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creare un grafo vuoto\n",
    "G = nx.Graph()\n",
    "\n",
    "Threshold = 0 #threshold similarità\n",
    "\n",
    "# Itera su ogni record nel file JSON\n",
    "for item in data:\n",
    "    dictionary = dict(item.items())\n",
    "    if not G.has_node(dictionary[\"user1\"]): #se utente non presente, lo aggiungo alla rete\n",
    "        G.add_node(dictionary[\"user1\"])\n",
    "    if not G.has_node(dictionary[\"user2\"]): #se utente non presente, lo aggiungo alla rete\n",
    "        G.add_node(dictionary[\"user2\"])\n",
    "    if float(dictionary[\"similarity\"])>Threshold:\n",
    "        G.add_edge(dictionary[\"user1\"], dictionary[\"user2\"], weight=float(dictionary[\"similarity\"]))\n",
    "    \n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77a762",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disegnare il grafo\n",
    "pos = nx.spring_layout(G)  # Posizionamento dei nodi\n",
    "weights = nx.get_edge_attributes(G, 'weight').values()\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=5, font_size=5, font_weight='bold')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{d[\"weight\"]:.2f}' for u, v, d in G.edges(data=True)}, font_color='red')\n",
    "nx.draw_networkx_edges(G, pos, width=list(weights))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 935914,
     "sourceId": 1632066,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-14T14:07:11.387989",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}